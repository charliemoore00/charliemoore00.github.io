{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Implementing Logistic Regression [in progress]\n",
    "author: Charlie Moore\n",
    "date: '2024-05-12'\n",
    "image: \"image.jpg\"\n",
    "description: \"'Implementing Logistic Regression\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from logistic import LogisticRegression, GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recently, we introduced the gradient descent algorithm for solving the empirical risk minimization problem. We also calculated the gradient of the loss function for logistic regression.\n",
    "\n",
    "In this blog post you will:\n",
    "\n",
    "Implement gradient descent for logistic regression in an object-oriented paradigm.\n",
    "Implement a key variant of gradient descent with momentum in order to achieve faster convergence.\n",
    "Perform experiments to test your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Implement Logistic Regression\n",
    "\n",
    "## Implement ``LinearModel`` and ``LogisticRegression()``\n",
    "If you havenâ€™t already, implement the methods of the ``LinearModel`` class as described in this warmup. Then, define a new class called ``LogisticRegression`` which inherits from ``LinearModel``. This class should have two methods:\n",
    "\n",
    "- ``LogisticRegression.loss(X, y)`` should compute the empirical risk $L(w)$ using the logistic loss function. The weight vector ``w`` used for this calculation should be stored as an instance variable of the class.\n",
    "\n",
    "- ``LogisticRegression.grad(X, y)`` should compute the gradient of the empirical risk $L(w)$. You can use the formula for the gradient supplied in the lecture notes on gradient descent.\n",
    "\n",
    "For an **M**, you can implement ``LogisticRegression.grad`` using a ``for``-loop. For an **E**, your solution should involve no explicit loops. While working on a solution that avoids loops, you might find it useful to at some point convert a tensor ``v`` with shape ``(n,)`` into a tensor ``v_`` with shape ``(n,1)``. The code ``v_ = v[:, None]`` will perform this conversion for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ``GradientDescentOptimizer``\n",
    "\n",
    "Next, implement a ``GradientDescentOptimizer`` class. For this project, we are going to implement _gradient descent with momentum_, also known as Spicy Gradient Descent. Let w~k~ be the estimate of the weight vector at algorithmic step k. Gradient descent with momentum performs the update equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Experiments\n",
    "\n",
    "## Experimental Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5752)\n",
      "tensor([-0.1069, -0.0829,  0.1401])\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "def classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n",
    "    \n",
    "    y = torch.arange(n_points) >= int(n_points/2)\n",
    "    y = 1.0*y\n",
    "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
    "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = classification_data(noise = 0.5)\n",
    "\n",
    "LR = LogisticRegression()\n",
    "opt = GradientDescentOptimizer(LR)\n",
    "s = LR.loss(X, y)\n",
    "g = LR.grad(X, y)\n",
    "print(s)\n",
    "print(g)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How to Train Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN the model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "LR = LogisticRegression() \n",
    "opt = GradientDescentOptimizer(LR)\n",
    "\n",
    "loss_vec = []\n",
    "\n",
    "for _ in range(100):\n",
    "    # add other stuff to e.g. keep track of the loss over time. \n",
    "\n",
    "    loss = LR.loss(X, y)\n",
    "    loss_vec.append(loss)\n",
    "\n",
    "    opt.step(X, y, alpha = 0.1, beta = 0.9)\n",
    "\n",
    "plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n",
    "plt.semilogx()\n",
    "labs = plt.gca().set(xlabel = \"Number of Gradient Descent iterations\", ylabel=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Writing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
