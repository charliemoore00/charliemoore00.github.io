[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Charlie, a CS student graduating this May from Middlebury College.\nThis website is dedicated to showing my learning in this semester’s Machine Learning course through a series of blog posts - these are short research projects aimed at polishing our skills working with data and constructing models, as well as thinking about the social and cultural impacts and bias present in the machine learning world."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Penguins Blog",
    "section": "",
    "text": "The aim of this analysis is to explore and analyze the Palmer Penguins dataset and construct a classifier that can predict the species of penguin (Adelie, Gentoo, Chinstrap) based on the other factors in the data. The findings revealed that a logistic regression model performed the best in predicting the penguin species based on selected features. The model achieved a high accuracy score of 99.6% on the training data and 100% on the test data. We also found the determining features are culmen length, culmen depth, and island location that allow us to classify the penguin species.\nFirst, we’ll load the palmer penguins dataset:\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(url)\n\nWe’ll take a quick peek at how the data looks:\n\ntrain.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#explore",
    "href": "posts/penguins-blog-post/index.html#explore",
    "title": "Penguins Blog",
    "section": "Explore",
    "text": "Explore\n\nCreating visualizations to learn about our dataset.\nThe following table shows the means, minimums, and maximums of each column in the dataset, grouped by island. One thing to point out is that the mean for “Stage_Adult, 1 Egg Stage” is 1.0, therefore it won’t help us predict the species. No need to include it in future code!\nAlso, the means for the Delta values are very similar across the islands, so we can guess that if we include island in our model, the delta values won’t be very helpful in predicting the species.\n\npd.set_option('display.max_columns', None)\nX_train.groupby([\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]).agg([\"mean\", \"min\", \"max\"])\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n1\n39.350000\n34.4\n46.0\n18.441176\n15.9\n21.5\n192.235294\n176.0\n210.0\n3727.941176\n2900.0\n4700.0\n8.844635\n7.69778\n9.59462\n-25.748135\n-26.53870\n-23.90309\n1.0\n1\n1\n0.205882\n0\n1\n0.794118\n0\n1\n0.470588\n0\n1\n0.529412\n0\n1\n\n\n1\n0\n44.527835\n34.0\n58.0\n18.307216\n16.4\n21.2\n193.628866\n178.0\n212.0\n3734.793814\n2700.0\n4800.0\n9.163175\n8.01485\n10.02544\n-25.075329\n-26.69543\n-23.89017\n1.0\n1\n1\n0.123711\n0\n1\n0.876289\n0\n1\n0.525773\n0\n1\n0.474227\n0\n1\n\n\n1\n0\n0\n44.945600\n34.5\n55.9\n15.863200\n13.1\n21.1\n209.320000\n172.0\n230.0\n4702.000000\n2850.0\n6300.0\n8.394096\n7.63220\n9.79532\n-26.086192\n-27.01854\n-24.36130\n1.0\n1\n1\n0.072000\n0\n1\n0.928000\n0\n1\n0.520000\n0\n1\n0.480000\n0\n1\n\n\n\n\n\n\n\n\n\nHeatmap\nThe following visualization shows the correlation between different pairs of variables. Values close to 1 or -1 indicate more linear relationships. Given that there are three penguin species, the most helpful visual is one that shows three distinct clumps. This would then likely not be a linear correlation, and so values that are closer to zero may show clumped data. We can see some of the lowest correlations are Culmen Depth vs Culmen Length, Body Mass vs Culmen Depth, and Body Mass vs Culmen Length.\n\ncorr = X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\n\n\n\n\n\n\n\n\n\n\nScatterplot\nThe following scatterplot shows the relationship between Culmen Depth (mm) on the X-axis and Body Mass (g) on the Y-axis. The scatterplot is colored by the sex of the penguin, with purple representing female and orange representing male. This scatterplot shows two distinct clusters of penguins, one in the upper left and the other bottom right. Since we have color-coded by gender, we can see that the clusters are not separated by gender! Perhaps the are separated by Species, in which case this relationship will be very important when creating a classification model. It’s likely that one of the penguin species is represented by the cluster in the upper left, and the other two are represented by the cluster in the bottom right. I make this assumption given that the lower right cluster is roughly twice the size of the other.\n\nsns.scatterplot(X_train, x=\"Culmen Depth (mm)\", y=\"Body Mass (g)\", hue=\"Sex_MALE\", palette=\"CMRmap\")"
  },
  {
    "objectID": "posts/whose-costs/index.html",
    "href": "posts/whose-costs/index.html",
    "title": "Whose Costs? [in progress]",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nPart A: Grab the Data\nWe’ll start by loading the data into a dataframe:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ntrain = pd.read_csv(url)\n\nWe’ll take a peak at how the data looks:\n\ntrain.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nPart B: Explore The Data\nCreate at least two visualizations and one summary table in which you explore patterns in the data. You might consider some questions like:\nHow does loan intent vary with the age, length of employment, or homeownership status of an individual?\nWhich segments of prospective borrowers are offered low interest rates? Which segments are offered high interest rates?\nWhich segments of prospective borrowers have access to large lines of credit?\n\nSummary table\nThe following table shows the means of each quantitative column, grouped by loan_grade, descending from the best grade ‘A’, denoting confidence that the recipient will pay back the loan, to the worst grade ‘G’, denoting the least confidence that the recipient will pay back the loan. We can see that loan as percent of income increases as the loan grade decreases. Loan amount increases as the loan grade decreases. Loan interest rate increases as the loan grade decreases.\nInteresting: the lower grades have a higher income.\nI’ve selected just the columns with quantitative information and cleaned the data by removing rows with missing values.\n\nquant_col = [\"loan_grade\", \"loan_percent_income\", \"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"cb_person_cred_hist_length\"]\ntrain_quant = train[quant_col].dropna()\ntrain_quant.groupby([\"loan_grade\"]).aggregate(\"mean\") #, \"person_home_ownership\"\n\n\n\n\n\n\n\n\nloan_percent_income\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\ncb_person_cred_hist_length\n\n\nloan_grade\n\n\n\n\n\n\n\n\n\n\n\nA\n0.152629\n27.682077\n66773.007816\n5.122533\n8555.884885\n7.338368\n5.730560\n\n\nB\n0.173846\n27.673949\n66662.091096\n4.750069\n10031.025007\n11.003273\n5.766007\n\n\nC\n0.168928\n27.792667\n66416.633130\n4.402663\n9322.102794\n13.456237\n5.855303\n\n\nD\n0.188833\n27.853112\n64555.473908\n4.699652\n10821.646695\n15.358261\n5.861229\n\n\nE\n0.204190\n27.732955\n70868.349432\n4.458807\n12929.083807\n17.047344\n5.747159\n\n\nF\n0.220982\n28.564417\n80756.546012\n4.239264\n15395.705521\n18.519018\n6.214724\n\n\nG\n0.243409\n28.181818\n77342.477273\n5.954545\n17384.659091\n20.230000\n6.500000\n\n\n\n\n\n\n\nLet’s load our visualization library:\n\nimport seaborn as sns\n\n\n\nVisualization 1\nThis visualization shows the effect of loan grade on loan amount in two cases - if the person defaulted on their loan or not. We can see that as the loan amount increases, the loan grade decreases. This indicates loan amount may be a good predictor variable of loan status. While a default on file doesn’t have much effect on the loan amount, we can see that there are NO loans of grades A and B given to those with a default on file. This is an interesting bit of information that could definitely help predict loan status - on the other hand, it may be more of a mistrustful approach for the bank not wanting to lose money again, therefore skewing their loan grade variable.\n\nsns.boxplot(train, x=\"cb_person_default_on_file\", y=\"loan_amnt\", hue_order=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"], hue=\"loan_grade\", palette=\"Set2\", fliersize=0.5)\n\n\n\n\n\n\n\n\n\n\nVisualization 2\nHere we visualize a scatterplot where loan percent income is on the x-axis, and loan interest rate is on the y-axis. The color of the data corresponds to the loan grade given by the bank.\nWe can see that loan interest rate has a high correlation to loan grade - the higher the interest rate, the lower the loan grade.\n\norder_loan_grade = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\nsns.scatterplot(train, x=\"loan_percent_income\", y=\"loan_int_rate\", hue=\"loan_grade\", hue_order=order_loan_grade)\n\n\n\n\n\n\n\n\n\n\n\nPart C: Build a Model\nWe’re going to use cross validation to score all combinations of features to find the best ones to use in our model.\n\nData Preparation\nIn order to find the optimal variables for a model, we need to prep the data - specifically dropping rows with NAs and using one-hot encoding to turn the categorical variables into numerical ones.\n\ntrain = train.dropna()\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"loan_status\"])\n\ndef prepare_data(df):\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = 1*pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIf we check how our data looks, we can see each categorical variable now is represented by multiple columns with binary values.\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_VENTURE\nloan_grade_A\nloan_grade_B\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n5 rows × 26 columns\n\n\n\nWe will use the combinations function from the itertools package. This will list all the combinations of one discreet variable and two continuous variables. Iterating through all the possible combinations, it will A) score the model using cross validation, and B) return the best combination of columns that performed the best.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\n\nLR = LogisticRegression(max_iter=999)\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nscore = 0\ncols_best = []\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n\n        #fit the models\n        log_score = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n        #compare the scores\n        if log_score &gt; score:\n            score = log_score\n            cols_best = cols\n\n#output the best score, along with the corresponding columns\nprint(score)\nprint(cols_best)\n\n0.8488229950993185\n['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']\n\n\nLooks like the best features to use are: loan_intent, person_age, and loan_percent_income\n\nLR = LogisticRegression(max_iter=999999999)\nmodel = LR.fit(X_train[cols_best], y_train)\n\n\n# our weights are now stored in:\nmodel.coef_\n\narray([[-7.52981385e-01, -8.75812948e-02, -1.79583155e+00,\n         2.81498971e-01, -4.00881551e-03,  8.27632650e+00]])\n\n\n\nX_train[cols_best]\n\n\n\n\n\n\n\n\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nperson_age\nloan_percent_income\n\n\n\n\n1\n0\n0\n0\n1\n27\n0.12\n\n\n2\n0\n0\n0\n1\n22\n0.27\n\n\n3\n0\n0\n0\n1\n24\n0.05\n\n\n4\n1\n0\n0\n0\n29\n0.28\n\n\n6\n0\n0\n0\n1\n21\n0.25\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n1\n0\n0\n0\n36\n0.02\n\n\n26060\n0\n0\n0\n1\n23\n0.09\n\n\n26061\n0\n0\n0\n1\n22\n0.25\n\n\n26062\n1\n0\n0\n0\n30\n0.24\n\n\n26063\n0\n0\n0\n1\n25\n0.36\n\n\n\n\n22907 rows × 6 columns\n\n\n\n\n\n\nPart D: Find a Threshold\nOnce you have a weight vector w, it is time to choose a threshold t. To choose a threshold that maximizes profit for the bank, we need to make some assumptions about how the bank makes and loses money on loans. Let’s use the following (simplified) modeling assumptions:\nIf the loan is repaid in full, the profit for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt. This formula assumes that the profit earned by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going to things like salaries for the people who manage the bank. It is extremely simplistic and does not account for inflation, amortization over time, opportunity costs, etc. If the borrower defaults on the loan, the “profit” for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt. This formula corresponds to the same profit-earning mechanism as above, but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. These modeling assumptions are extremely simplistic! You may deviate from these assumptions if you have relevant prior knowledge to inform your approach!!\nBased on your assumptions, determine the threshold t which optimizes profit for the bank on the training set. Explain your approach, including labeled visualizations where appropriate, and include a final estimate of the bank’s expected profit per borrower on the training set.\nLet’s first create a function to calculate the profit from predicted target variables. It will take in the prediction column of 0s and 1s that guesses if a person will default or not, and return the total profit.\n\ndef profit(target_col):\n    return (train[\"loan_amnt\"] * (1 + 0.0025*train[\"loan_int_rate\"])**(10 - 7*target_col) - (1 + 0.7*target_col)*train[\"loan_amnt\"]).sum()\n\nprofit(train[\"loan_status\"])\n\n18753738.0082548\n\n\nWe can see that the total profit is $18,753,738. But our job is to find the threshold t that maximizes this!\nFirst, we’ll turn our scores into a numpy array w for ease\n\nw = np.array(model.coef_)[0]\nw\n\narray([-7.52981385e-01, -8.75812948e-02, -1.79583155e+00,  2.81498971e-01,\n       -4.00881551e-03,  8.27632650e+00])\n\n\nLet’s create a function linear_score that computes the weighted score using our 6 predictor columns and weights w\n\ndef linear_score(X, w):\n    return X@w\n\nLet’s also create a predict method to guess based off of our threshold:\n\ndef predict(w, threshold, df):\n    scores = linear_score(df, w)\n\n    # debug plot\n    #hist = plt.hist(scores)\n    #labs=plt.gca().set(xlabel=r\"Score $score$\", ylabel=\"Frequency\")\n\n    return 1*(scores&gt;threshold)\n\nWe’ll use this function to add a new column into our dataframe of our predictions based off of our threshold. Let’s start off with a guess of 1 as the threshold:\n\ntrain[\"prediction\"] = predict(w, 1, X_train[cols_best])\n\nLet’s see how well our guess does!\n\n(train[\"prediction\"] == train[\"loan_status\"]).mean()\n\n0.6271881957480246\n\n\nThat reflects how accurate we were - but how about finding the profit from that threshold? That’s ultimately the goal here.\n\nprofit(train[\"prediction\"])\n\n-46485038.05424925\n\n\nOof! Looks like this would result in the bank losing over $46 million! Now let’s find a better threshold that allows our bank to make the most profit.\nLet’s create a master function optimal that takes in a threshold t and returns the profit\n\ndef optimal(t):\n    return profit(predict(w, t, X_train[cols_best]))\n\n# test it on a threshold of 1\nprint(optimal(1))\n\n-46485038.05424925\n\n\nNow let’s graph this function over the span of possible thresholds\n\nt_values = np.linspace(-6, 8, 200)\noptimal_values = [optimal(t) for t in t_values]\n\n# Plotting the function\nplt.figure(figsize=(10, 5))\nplt.plot(t_values, optimal_values, label='Optimal(t)', color='blue')\nplt.title('Graph of Optimal(t) from -8 to 8')\nplt.xlabel('t')\nplt.ylabel('Optimal(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRequest for feedback:\n\nI’m not sure why the optimal threshold function (with profit on the y-axis) is yielding a sigmoid function. I would assume there should be a local maximum - in this case, the optimal threshold would be infinite, and the bank would accept every loan. I’m extremely doubtful that this is the case, so I’m wondering what I’m doing wrong exactly that’s yielding this graph.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\nNow we’ll use gradient descent to find the optimal value for the threshold\n\nfrom scipy.optimize import minimize\n\ninit_threshold = 1\n#result = minimize(lambda t: optimal(t), x0 = init_threshold, bounds = [(-6, 8)], method='BFGS')\n\n#optimal_threshold = result.x\n#print(\"Optimal threshold: \", result)\n#print(\"Maximum Profit: \", -optimal(optimal_threshold))\n\n\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\nOnly after you have finalized your weight w vector and threshold t, evaluate your automated decision-process on the test set:\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ntrain_test = pd.read_csv(url)\n\nWhat is the expected profit per borrower on the test set? Is it similar to your profit on the training set?\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\nNow evaluate your model from the (aggregate) perspective of the prospective borrowers. Please quantitatively address the following questions, using the predictions of your model on the test data:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\n\nPart G: Write and Reflect\nWrite a brief introductory paragraph for your blog post describing the overall purpose, methodology, and findings of your study. Then, write a concluding discussion describing what you found and what you learned through from this blog post.\nPlease include one paragraph discussing the following questions:\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit? You are free to define “fairness” in a way that makes sense to you, but please write down your definition as part of your discussion."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Gradient Descent for Logistic Regression is implemented in logistic.py"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#abstract",
    "href": "posts/implementing-logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nThis blog post contains an implementation of Logistic Regression with a Gradient Descent with Momentum Optimizer. The implementation exists as the logistic.py class, available above. We’ll set up some sample data, train a model, and conduct some experiments using this implementation:\n\nClassic Gradient Descent without momentum: using just two features\nGradient Descent with momentum: how does momentum affect how quickly and accurately our model converges?\nOverfitting: how does the relationship between number of data points and number of features affect our model’s accuracy?\n\nFirst, let’s load our logistic regression implementation and other necessary packages.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt # type: ignore\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#introduction",
    "href": "posts/implementing-logistic-regression/index.html#introduction",
    "title": "Implementing Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nIn this post we will:\n\nImplement gradient descent for logistic regression in an object-oriented paradigm.\nImplement a key variant of gradient descent with momentum in order to achieve faster convergence.\nPerform experiments to test the implementations."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#generating-experimental-data",
    "href": "posts/implementing-logistic-regression/index.html#generating-experimental-data",
    "title": "Implementing Logistic Regression",
    "section": "Generating Experimental Data",
    "text": "Generating Experimental Data\nHere is a method to generate data. The parameters are: - n_points: the number of points - p_dims: the number of features - noise: the difficulty of the classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX_rand, y_rand = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#training-the-model",
    "href": "posts/implementing-logistic-regression/index.html#training-the-model",
    "title": "Implementing Logistic Regression",
    "section": "Training the Model",
    "text": "Training the Model\nWe’ll create a function that trains the model by iterating over the optimizer’s ‘step’ function.\n\ndef train_and_plot(X, y, alpha, beta, iterations=100):\n\n    # initialize a ligistic regression and optimizer\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # to keep track of the loss to plot it\n    loss_vec = []\n\n    for _ in range(iterations):\n\n       # keep track of the loss over time. \n        loss = LR.loss(X, y)\n        loss_vec.append(loss)\n\n        # use GradientDescentOptimizer's step function\n        opt.step(X, y, alpha, beta)\n\n\n    # plot the loss\n    plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n    plt.semilogx()\n    labs = plt.gca().set(xlabel = \"Number of Gradient Descent iterations\", ylabel=\"loss\")"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#conducting-some-experiments",
    "href": "posts/implementing-logistic-regression/index.html#conducting-some-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Conducting Some Experiments!",
    "text": "Conducting Some Experiments!\n\n1. Vanilla gradient descent:\nWhen the number of features p_dim = 2, when alpha is sufficiently small and beta=0, gradient descent for logistic regression converges to a weight vector w that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations).\nFirst, lets generate some experimental data with 2 dimensions:\n\nX_exp, y_exp = classification_data(n_points = 300, noise = 0.5, p_dims = 2)\n\nNext, let’s run our train_and_plot method with a sufficiently small value (0.1) for alpha, and with beta = 0.\nThis is a basic version of gradient descent without momentum (the beta term).\n\nHaving a small alpha value, or learning rate, means that our gradient will converge smoothly without any big jumps or overshooting the minimum.\nHaving a beta value of 0 means that the last term in the GradientDescentOptimizer.step function will evaluate to 0. This means it won’t take into account the old weight vector and add momentum to the term when updating the new weights.\n\nWith these parameters, we should see a smooth curve with the loss constantly decreasing as our step function iterates:\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0)\n\n\n\n\n\n\n\n\n\n\n2. Benefits of momentum:\nOn the same data, gradient descent with momentum (e.g. beta = 0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta = 0). Plot the loss over iterations for each method.\nWe’ll use the same data (x_exp, y_exp) to show what simply adding momentum can do. By changing our beta term, it adds a whole new dimension for equation (2) in logistic.py. The equation\n\\[\\begin{aligned}\n    \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\\]\nwill now have a non-zero last term with beta, which incorporates past weights \\({w}_{k-1}\\) to add momentum.\nBy including past weights and present weights to calculate future weights, the momentum term helps converge faster, and we can see this by using the train_and_plot function. Again, the only thing we’ve changed is the beta term from 0 to 0.9\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0.9)\n\n\n\n\n\n\n\n\n\n\nOverfitting:\nLet’s generate some data where p_dim &gt; n_points. We’ll do this twice with the exact same parameters, the first for a train set and the second for a test set.\n\npoints = 200\ndims = 1000\n\nX_train, y_train = classification_data(n_points=points, noise=0.5, p_dims=dims)\nX_test, y_test = classification_data(n_points=points, noise=0.5, p_dims=dims)\n\nWe’ll then do an experiment in which we fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data.\n\n# create our logistic regression and optimizer\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_acc = 0\n\n# train until we achieve 100% accuracy :0\nwhile (train_acc != 1):\n    \n    opt.step(X_train, y_train, 0.1, 0.9)\n\n    # use LogisticRegression.predict method to calculate accuracy\n    train_acc = (1.0*(LR.predict(X_train)==y_train)).mean()\n\n\nprint(\"training accuracy: \", float(train_acc))\nprint(\"testing accuracy: \", float((1.0*(LR.predict(X_test) == y_test)).mean()))\n\ntraining accuracy:  1.0\ntesting accuracy:  0.7900000214576721\n\n\nWhile it depends to a certain degree on the level of noise in the data, we can see that our testing accuracy is considerable lower than training accuracy. This is because using data where the number of features outweighs the number of data points results in overfitting.\nBecause there are so many features, the logistic regression model will essential memorize the data, rather than generalize from it - this is the way overfitting works. On new test data the model hasn’t seen, it isn’t as equipped to generalize to these new data points.\nIf we use the exact same pipeline, but switch the number of dimensions with the number of data points, we can see our test accuracy skyrocket.\n\npoints = 1000\ndims = 200\n\nX_train, y_train = classification_data(n_points=points, noise=0.5, p_dims=dims)\nX_test, y_test = classification_data(n_points=points, noise=0.5, p_dims=dims)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\ntrain_acc = 0\n\nwhile (train_acc != 1):\n    \n    opt.step(X_train, y_train, 0.1, 0.9)\n    train_acc = (1.0*(LR.predict(X_train)==y_train)).mean()\n\nprint(\"training accuracy: \", float(train_acc))\nprint(\"testing accuracy: \", float((1.0*(LR.predict(X_test) == y_test)).mean()))\n\ntraining accuracy:  1.0\ntesting accuracy:  0.9890000224113464"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#discussion",
    "href": "posts/implementing-logistic-regression/index.html#discussion",
    "title": "Implementing Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we implemented Logistic Regression and a Gradient Descent with Momentum optimizer. This was a great way to cement my knowledge about logistic regression and learn the intricacies of the loss, grad, and step functions necessary to make these models and training loops work. The experiments showed the effects of momentum (the beta value), the relationship between number of points and number of dimensions, and visualized how Gradient Descent minimizes loss over the iteration cycle. I can now confidently describe the parameters, inner workings, and outputs of Gradient Descent."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\n’Implementing Logistic Regression\n\n\n\n\n\nMay 12, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs? [in progress]\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nMar 7, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Blog\n\n\n\n\n\nPalmer Penguins!\n\n\n\n\n\nFeb 19, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#part-a-implementing-logistic-regression",
    "href": "posts/implementing-logistic-regression/index.html#part-a-implementing-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Part A: Implementing Logistic Regression",
    "text": "Part A: Implementing Logistic Regression\nThe following classes are implemented in logistic.py - the link is at the top of this page.\n\nImplement LinearModel, LogisticRegression(), and GradientDescentOptimizer\nThe LogisticRegression() class has two methods:\n\nLogisticRegression.loss(X, y) computes the empirical risk \\(L(w)\\) using the logistic loss function:\n\\[\\begin{aligned}\nL(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right]\n\\end{aligned}\\]\n.\nLogisticRegression.grad(X, y) computes the gradient of the empirical risk \\({\\nabla}L(w)\\) using this formula:\n\\[\\begin{aligned}\n\\nabla L(\\mathbf{w}) &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle) - y_i)\\mathbf{x}_i\\;\n\\end{aligned}\\]\n.\n\nThe GradientDescentOptimizer class has one method:\n\nGradientDescentOptimizer.step(X, y, alpha, beta) updates the weight vector. At algorithmic step k, it computes new weights:\n\\[\\begin{aligned}\n  \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\\]\nHere, alpha and beta are two learning rate parameters. When beta = 0 we have “regular” gradient descent. In practice, a choice of beta = 0.9 is common."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#implementing-logistic-regression",
    "href": "posts/implementing-logistic-regression/index.html#implementing-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Implementing Logistic Regression",
    "text": "Implementing Logistic Regression\nThe following classes are implemented in logistic.py - the link is at the top of this page.\n\nImplement LinearModel, LogisticRegression(), and GradientDescentOptimizer\nThe LogisticRegression() class has two methods:\n\nLogisticRegression.loss(X, y) computes the empirical risk \\(L(w)\\) using the logistic loss function:\n\\[\\begin{aligned}\nL(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right]\n\\end{aligned}\\]\n.\nLogisticRegression.grad(X, y) computes the gradient of the empirical risk \\({\\nabla}L(w)\\) using this formula:\n\\[\\begin{aligned}\n\\nabla L(\\mathbf{w}) &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle) - y_i)\\mathbf{x}_i\\;\n\\end{aligned}\\]\n.\n\nThe GradientDescentOptimizer class has one method:\n\nGradientDescentOptimizer.step(X, y, alpha, beta) updates the weight vector. At algorithmic step k, it computes new weights:\n\\[\\begin{aligned}\n  \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\\]\nHere, alpha and beta are two learning rate parameters. When beta = 0 we have “regular” gradient descent. In practice, a choice of beta = 0.9 is common."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#experiments",
    "href": "posts/implementing-logistic-regression/index.html#experiments",
    "title": "Implementing Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nGenerating Experimental Data\nHere is a method to generate data. The parameters are:\n\nn_points: the number of points\np_dims: the number of features\nnoise: the difficulty of the classification problem\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX_rand, y_rand = classification_data(noise = 0.5)\n\n\n\nTraining the Model\nWe’ll create a function that trains the model by iterating over the optimizer’s ‘step’ function.\n\ndef train_and_plot(X, y, alpha, beta, iterations=100):\n\n    # initialize a ligistic regression and optimizer\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # to keep track of the loss to plot it\n    loss_vec = []\n\n    for _ in range(iterations):\n\n       # keep track of the loss over time. \n        loss = LR.loss(X, y)\n        loss_vec.append(loss)\n\n        # use GradientDescentOptimizer's step function\n        opt.step(X, y, alpha, beta)\n\n\n    # plot the loss\n    plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n    plt.semilogx()\n    labs = plt.gca().set(xlabel = \"Number of Gradient Descent iterations\", ylabel=\"loss\")"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#conduct-some-experiments",
    "href": "posts/implementing-logistic-regression/index.html#conduct-some-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Conduct Some Experiments!",
    "text": "Conduct Some Experiments!\n\n1. Vanilla gradient descent:\nWhen the number of features p_dim = 2, alpha is sufficiently small, and beta=0, gradient descent for logistic regression converges to a weight vector w that looks visually correct and the loss decreases monotonically.\nFirst, lets generate some experimental data with 2 dimensions:\n\nX_exp, y_exp = classification_data(n_points = 300, noise = 0.5, p_dims = 2)\n\nNext, let’s run our train_and_plot method with a sufficiently small value (0.1) for alpha, and with beta = 0.\nThis is a basic version of gradient descent without momentum (the beta term).\n\nHaving a small alpha value, or learning rate, means that our gradient will converge smoothly without any big jumps or overshooting the minimum.\nHaving a beta value of 0 means that the last term in the GradientDescentOptimizer.step function will evaluate to 0. This means it won’t take into account the old weight vector and add momentum to the term when updating the new weights.\n\nWith these parameters, we should see a smooth curve with the loss constantly decreasing as our step function iterates:\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0)\n\n\n\n\n\n\n\n\n\n\n2. Benefits of momentum:\nOn the same data, gradient descent with momentum (e.g. beta = 0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta = 0). Plot the loss over iterations for each method.\nWe’ll use the same data (x_exp, y_exp) to show what simply adding momentum can do. By changing our beta term, it adds a whole new dimension for equation (2) in logistic.py. The equation\n\\[\\begin{aligned}\n    \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\\]\nwill now have a non-zero last term with beta, which incorporates past weights \\({w}_{k-1}\\) to add momentum.\nBy including past weights and present weights to calculate future weights, the momentum term helps converge faster, and we can see this by using the train_and_plot function. Again, the only thing we’ve changed is the beta term from 0 to 0.9\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0.9)\n\n\n\n\n\n\n\n\n\n\nOverfitting:\nLet’s generate some data where p_dim &gt; n_points. We’ll do this twice with the exact same parameters, the first for a train set and the second for a test set.\n\npoints = 200\ndims = 1000\n\nX_train, y_train = classification_data(n_points=points, noise=0.5, p_dims=dims)\nX_test, y_test = classification_data(n_points=points, noise=0.5, p_dims=dims)\n\nWe’ll then do an experiment in which we fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data.\n\n# create our logistic regression and optimizer\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_acc = 0\n\n# train until we achieve 100% accuracy :0\nwhile (train_acc != 1):\n    \n    opt.step(X_train, y_train, 0.1, 0.9)\n\n    # use LogisticRegression.predict method to calculate accuracy\n    train_acc = (1.0*(LR.predict(X_train)==y_train)).mean()\n\n\nprint(\"training accuracy: \", float(train_acc))\nprint(\"testing accuracy: \", float((1.0*(LR.predict(X_test) == y_test)).mean()))\n\ntraining accuracy:  1.0\ntesting accuracy:  0.7900000214576721\n\n\nWhile it depends to a certain degree on the level of noise in the data, we can see that our testing accuracy is considerable lower than training accuracy. This is because using data where the number of features outweighs the number of data points results in overfitting.\nBecause there are so many features, the logistic regression model will essential memorize the data, rather than generalize from it - this is the way overfitting works. On new test data the model hasn’t seen, it isn’t as equipped to generalize to these new data points.\nIf we use the exact same pipeline, but switch the number of dimensions with the number of data points, we can see our test accuracy skyrocket.\n\npoints = 1000\ndims = 200\n\nX_train, y_train = classification_data(n_points=points, noise=0.5, p_dims=dims)\nX_test, y_test = classification_data(n_points=points, noise=0.5, p_dims=dims)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\ntrain_acc = 0\n\nwhile (train_acc != 1):\n    \n    opt.step(X_train, y_train, 0.1, 0.9)\n    train_acc = (1.0*(LR.predict(X_train)==y_train)).mean()\n\nprint(\"training accuracy: \", float(train_acc))\nprint(\"testing accuracy: \", float((1.0*(LR.predict(X_test) == y_test)).mean()))\n\ntraining accuracy:  1.0\ntesting accuracy:  0.9890000224113464"
  }
]