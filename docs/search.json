[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Charlie, a CS student graduating this May from Middlebury College.\nThis website is dedicated to showing my learning in this semester’s Machine Learning course through a series of blog posts - these are short research projects aimed at polishing our skills working with data and constructing models, as well as thinking about the social and cultural impacts and bias present in the machine learning world."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Penguins Blog",
    "section": "",
    "text": "The aim of this analysis is to explore and analyze the Palmer Penguins dataset and construct a classifier that can predict the species of penguin (Adelie, Gentoo, Chinstrap) based on the other factors in the data. The findings revealed that a logistic regression model performed the best in predicting the penguin species based on selected features. The model achieved a high accuracy score of 99.6% on the training data and 100% on the test data. We also found the determining features are culmen length, culmen depth, and island location that allow us to classify the penguin species.\nFirst, we’ll load the palmer penguins dataset:\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(url)\n\nWe’ll take a quick peek at how the data looks:\n\ntrain.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#explore",
    "href": "posts/penguins-blog-post/index.html#explore",
    "title": "Penguins Blog",
    "section": "Explore",
    "text": "Explore\n\nCreating visualizations to learn about our dataset.\nThe following table shows the means, minimums, and maximums of each column in the dataset, grouped by island. One thing to point out is that the mean for “Stage_Adult, 1 Egg Stage” is 1.0, therefore it won’t help us predict the species. No need to include it in future code!\nAlso, the means for the Delta values are very similar across the islands, so we can guess that if we include island in our model, the delta values won’t be very helpful in predicting the species.\n\npd.set_option('display.max_columns', None)\nX_train.groupby([\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]).agg([\"mean\", \"min\", \"max\"])\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n1\n39.350000\n34.4\n46.0\n18.441176\n15.9\n21.5\n192.235294\n176.0\n210.0\n3727.941176\n2900.0\n4700.0\n8.844635\n7.69778\n9.59462\n-25.748135\n-26.53870\n-23.90309\n1.0\n1\n1\n0.205882\n0\n1\n0.794118\n0\n1\n0.470588\n0\n1\n0.529412\n0\n1\n\n\n1\n0\n44.527835\n34.0\n58.0\n18.307216\n16.4\n21.2\n193.628866\n178.0\n212.0\n3734.793814\n2700.0\n4800.0\n9.163175\n8.01485\n10.02544\n-25.075329\n-26.69543\n-23.89017\n1.0\n1\n1\n0.123711\n0\n1\n0.876289\n0\n1\n0.525773\n0\n1\n0.474227\n0\n1\n\n\n1\n0\n0\n44.945600\n34.5\n55.9\n15.863200\n13.1\n21.1\n209.320000\n172.0\n230.0\n4702.000000\n2850.0\n6300.0\n8.394096\n7.63220\n9.79532\n-26.086192\n-27.01854\n-24.36130\n1.0\n1\n1\n0.072000\n0\n1\n0.928000\n0\n1\n0.520000\n0\n1\n0.480000\n0\n1\n\n\n\n\n\n\n\n\n\nHeatmap\nThe following visualization shows the correlation between different pairs of variables. Values close to 1 or -1 indicate more linear relationships. Given that there are three penguin species, the most helpful visual is one that shows three distinct clumps. This would then likely not be a linear correlation, and so values that are closer to zero may show clumped data. We can see some of the lowest correlations are Culmen Depth vs Culmen Length, Body Mass vs Culmen Depth, and Body Mass vs Culmen Length.\n\ncorr = X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\n\n\n\n\n\n\n\n\n\n\nScatterplot\nThe following scatterplot shows the relationship between Culmen Depth (mm) on the X-axis and Body Mass (g) on the Y-axis. The scatterplot is colored by the sex of the penguin, with purple representing female and orange representing male. This scatterplot shows two distinct clusters of penguins, one in the upper left and the other bottom right. Since we have color-coded by gender, we can see that the clusters are not separated by gender! Perhaps the are separated by Species, in which case this relationship will be very important when creating a classification model. It’s likely that one of the penguin species is represented by the cluster in the upper left, and the other two are represented by the cluster in the bottom right. I make this assumption given that the lower right cluster is roughly twice the size of the other.\n\nsns.scatterplot(X_train, x=\"Culmen Depth (mm)\", y=\"Body Mass (g)\", hue=\"Sex_MALE\", palette=\"CMRmap\")"
  },
  {
    "objectID": "posts/whose-costs/index.html",
    "href": "posts/whose-costs/index.html",
    "title": "Whose Costs? [in progress]",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\nPart A: Grab the Data\nWe’ll start by loading the data into a dataframe:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ntrain = pd.read_csv(url)\n\nWe’ll take a peak at how the data looks:\n\ntrain.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nPart B: Explore The Data\nCreate at least two visualizations and one summary table in which you explore patterns in the data. You might consider some questions like:\nHow does loan intent vary with the age, length of employment, or homeownership status of an individual?\nWhich segments of prospective borrowers are offered low interest rates? Which segments are offered high interest rates?\nWhich segments of prospective borrowers have access to large lines of credit?\n\nSummary table\nThe following table shows the means of each quantitative column, grouped by loan_grade, descending from the best grade ‘A’, denoting confidence that the recipient will pay back the loan, to the worst grade ‘G’, denoting the least confidence that the recipient will pay back the loan. We can see that loan as percent of income increases as the loan grade decreases. Loan amount increases as the loan grade decreases. Loan interest rate increases as the loan grade decreases.\nInteresting: the lower grades have a higher income.\nI’ve selected just the columns with quantitative information and cleaned the data by removing rows with missing values.\n\nquant_col = [\"loan_grade\", \"loan_percent_income\", \"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"cb_person_cred_hist_length\"]\ntrain_quant = train[quant_col].dropna()\ntrain_quant.groupby([\"loan_grade\"]).aggregate(\"mean\") #, \"person_home_ownership\"\n\n\n\n\n\n\n\n\nloan_percent_income\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\ncb_person_cred_hist_length\n\n\nloan_grade\n\n\n\n\n\n\n\n\n\n\n\nA\n0.152629\n27.682077\n66773.007816\n5.122533\n8555.884885\n7.338368\n5.730560\n\n\nB\n0.173846\n27.673949\n66662.091096\n4.750069\n10031.025007\n11.003273\n5.766007\n\n\nC\n0.168928\n27.792667\n66416.633130\n4.402663\n9322.102794\n13.456237\n5.855303\n\n\nD\n0.188833\n27.853112\n64555.473908\n4.699652\n10821.646695\n15.358261\n5.861229\n\n\nE\n0.204190\n27.732955\n70868.349432\n4.458807\n12929.083807\n17.047344\n5.747159\n\n\nF\n0.220982\n28.564417\n80756.546012\n4.239264\n15395.705521\n18.519018\n6.214724\n\n\nG\n0.243409\n28.181818\n77342.477273\n5.954545\n17384.659091\n20.230000\n6.500000\n\n\n\n\n\n\n\nLet’s load our visualization library:\n\nimport seaborn as sns\n\n\n\nVisualization 1\nThis visualization shows the effect of loan_grade on loan_amnt in two cases - if the person defaulted on their loan or not. We can see that\n\nsns.boxplot(train, x=\"cb_person_default_on_file\", y=\"loan_amnt\", hue_order=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"], hue=\"loan_grade\", palette=\"Set2\", fliersize=0.5)\n\n\n\n\n\n\n\n\n\n\nVisualization 2\nHere we visualize a scatterplot where loan percent income is on the x-axis, and loan interest rate is on the y-axis. The color of the data corresponds to the loan grade given by the bank.\nWe can see that loan interest rate has a high correlation to loan grade - the higher the interest rate, the lower the loan grade.\n\norder_loan_grade = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\nsns.scatterplot(train, x=\"loan_percent_income\", y=\"loan_int_rate\", hue=\"loan_grade\", hue_order=order_loan_grade)\n\n\n\n\n\n\n\n\n\n\n\nPart C: Build a Model\nWe’re going to use cross validation to score all combinations of features to find the best ones to use in our model.\n\nData Preparation\nIn order to find the optimal variables for a model, we need to prep the data - specifically dropping rows with NAs and using one-hot encoding to turn the categorical variables into numerical ones.\n\ntrain = train.dropna()\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"loan_status\"])\n\ndef prepare_data(df):\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = 1*pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIf we check how our data looks, we can see each categorical variable now is represented by multiple columns with binary values.\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_VENTURE\nloan_grade_A\nloan_grade_B\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n5 rows × 26 columns\n\n\n\nWe will use the combinations function from the itertools package. This will list all the combinations of one discreet variable and two continuous variables. Iterating through all the possible combinations, it will A) score the model using cross validation, and B) return the best combination of columns that performed the best.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\n\nLR = LogisticRegression(max_iter=999)\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nscore = 0\ncols_best = []\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n\n        #fit the models\n        log_score = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n        #compare the scores\n        if log_score &gt; score:\n            score = log_score\n            cols_best = cols\n\n#output the best score, along with the corresponding columns\nprint(score)\nprint(cols_best)\n\n0.8488229950993185\n['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']\n\n\nLooks like the best features to use are: loan_intent, person_age, and loan_percent_income\n\nLR = LogisticRegression(max_iter=999999999)\nmodel = LR.fit(X_train[cols_best], y_train)\n\n\n# our weights are now stored in:\nmodel.coef_\n\narray([[-7.52981385e-01, -8.75812948e-02, -1.79583155e+00,\n         2.81498971e-01, -4.00881551e-03,  8.27632650e+00]])\n\n\n\nX_train[cols_best]\n\n\n\n\n\n\n\n\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nperson_age\nloan_percent_income\n\n\n\n\n1\n0\n0\n0\n1\n27\n0.12\n\n\n2\n0\n0\n0\n1\n22\n0.27\n\n\n3\n0\n0\n0\n1\n24\n0.05\n\n\n4\n1\n0\n0\n0\n29\n0.28\n\n\n6\n0\n0\n0\n1\n21\n0.25\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n1\n0\n0\n0\n36\n0.02\n\n\n26060\n0\n0\n0\n1\n23\n0.09\n\n\n26061\n0\n0\n0\n1\n22\n0.25\n\n\n26062\n1\n0\n0\n0\n30\n0.24\n\n\n26063\n0\n0\n0\n1\n25\n0.36\n\n\n\n\n22907 rows × 6 columns\n\n\n\n\n\n\nPart D: Find a Threshold\nOnce you have a weight vector w, it is time to choose a threshold t. To choose a threshold that maximizes profit for the bank, we need to make some assumptions about how the bank makes and loses money on loans. Let’s use the following (simplified) modeling assumptions:\nIf the loan is repaid in full, the profit for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt. This formula assumes that the profit earned by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going to things like salaries for the people who manage the bank. It is extremely simplistic and does not account for inflation, amortization over time, opportunity costs, etc. If the borrower defaults on the loan, the “profit” for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt. This formula corresponds to the same profit-earning mechanism as above, but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal. These modeling assumptions are extremely simplistic! You may deviate from these assumptions if you have relevant prior knowledge to inform your approach!!\nBased on your assumptions, determine the threshold t which optimizes profit for the bank on the training set. Explain your approach, including labeled visualizations where appropriate, and include a final estimate of the bank’s expected profit per borrower on the training set.\nLet’s first create a function to calculate the profit from predicted target variables. It will take in the prediction column of 0s and 1s that guesses if a person will default or not, and return the total profit.\n\ndef profit(target_col):\n    return (train[\"loan_amnt\"] * (1 + 0.0025*train[\"loan_int_rate\"])**(10 - 7*target_col) - (1 + 0.7*target_col)*train[\"loan_amnt\"]).sum()\n\nprofit(train[\"loan_status\"])\n\n18753738.0082548\n\n\nWe can see that the total profit is $18,753,738. But our job is to find the threshold t that maximizes this!\nFirst, we’ll turn our scores into a numpy array w for ease\n\nw = np.array(model.coef_)[0]\nw\n\narray([-7.52981385e-01, -8.75812948e-02, -1.79583155e+00,  2.81498971e-01,\n       -4.00881551e-03,  8.27632650e+00])\n\n\nLet’s create a function linear_score that computes the weighted score using our 6 predictor columns and weights w\n\ndef linear_score(X, w):\n    return X@w\n\nLet’s also create a predict method to guess based off of our threshold:\n\ndef predict(w, threshold, df):\n    scores = linear_score(df, w)\n\n    # debug plot\n    #hist = plt.hist(scores)\n    #labs=plt.gca().set(xlabel=r\"Score $score$\", ylabel=\"Frequency\")\n\n    return 1*(scores&gt;threshold)\n\nWe’ll use this function to add a new column into our dataframe of our predictions based off of our threshold. Let’s start off with a guess of 1 as the threshold:\n\ntrain[\"prediction\"] = predict(w, 1, X_train[cols_best])\n\nLet’s see how well our guess does!\n\n(train[\"prediction\"] == train[\"loan_status\"]).mean()\n\n0.6271881957480246\n\n\nThat reflects how accurate we were - but how about finding the profit from that threshold? That’s ultimately the goal here.\n\nprofit(train[\"prediction\"])\n\n-46485038.05424925\n\n\nOof! Looks like this would result in the bank losing over $46 million! Now let’s find a better threshold that allows our bank to make the most profit.\nLet’s create a master function optimal that takes in a threshold t and returns the profit\n\ndef optimal(t):\n    return profit(predict(w, t, X_train[cols_best]))\n\n# test it on a threshold of 1\nprint(optimal(1))\n\n-46485038.05424925\n\n\nNow let’s graph this function over the span of possible thresholds\n\nt_values = np.linspace(-6, 8, 200)\noptimal_values = [optimal(t) for t in t_values]\n\n# Plotting the function\nplt.figure(figsize=(10, 5))\nplt.plot(t_values, optimal_values, label='Optimal(t)', color='blue')\nplt.title('Graph of Optimal(t) from -8 to 8')\nplt.xlabel('t')\nplt.ylabel('Optimal(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRequest for feedback:\n\nI’m not sure why the optimal threshold function (with profit on the y-axis) is yielding a sigmoid function. I would assume there should be a local maximum - in this case, the optimal threshold would be infinite, and the bank would accept every loan. I’m extremely doubtful that this is the case, so I’m wondering what I’m doing wrong exactly that’s yielding this graph.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\nNow we’ll use gradient descent to find the optimal value for the threshold\n\nfrom scipy.optimize import minimize\n\ninit_threshold = 1\n#result = minimize(lambda t: optimal(t), x0 = init_threshold, bounds = [(-6, 8)], method='BFGS')\n\n#optimal_threshold = result.x\n#print(\"Optimal threshold: \", result)\n#print(\"Maximum Profit: \", -optimal(optimal_threshold))\n\n\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\nOnly after you have finalized your weight w vector and threshold t, evaluate your automated decision-process on the test set:\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ntrain_test = pd.read_csv(url)\n\nWhat is the expected profit per borrower on the test set? Is it similar to your profit on the training set?\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\nNow evaluate your model from the (aggregate) perspective of the prospective borrowers. Please quantitatively address the following questions, using the predictions of your model on the test data:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\n\nPart G: Write and Reflect\nWrite a brief introductory paragraph for your blog post describing the overall purpose, methodology, and findings of your study. Then, write a concluding discussion describing what you found and what you learned through from this blog post.\nPlease include one paragraph discussing the following questions:\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit? You are free to define “fairness” in a way that makes sense to you, but please write down your definition as part of your discussion."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#introduction",
    "href": "posts/implementing-logistic-regression/index.html#introduction",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "Introduction",
    "text": "Introduction\nRecently, we introduced the gradient descent algorithm for solving the empirical risk minimization problem. We also calculated the gradient of the loss function for logistic regression.\nIn this blog post you will:\nImplement gradient descent for logistic regression in an object-oriented paradigm. Implement a key variant of gradient descent with momentum in order to achieve faster convergence. Perform experiments to test your implementations."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#implement-linearmodel-and-logisticregression",
    "href": "posts/implementing-logistic-regression/index.html#implement-linearmodel-and-logisticregression",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "Implement LinearModel and LogisticRegression()",
    "text": "Implement LinearModel and LogisticRegression()\nIf you haven’t already, implement the methods of the LinearModel class as described in this warmup. Then, define a new class called LogisticRegression which inherits from LinearModel. This class should have two methods:\n\nLogisticRegression.loss(X, y) should compute the empirical risk \\(L(w)\\) using the logistic loss function. The weight vector w used for this calculation should be stored as an instance variable of the class.\nLogisticRegression.grad(X, y) should compute the gradient of the empirical risk \\(L(w)\\). You can use the formula for the gradient supplied in the lecture notes on gradient descent.\n\nFor an M, you can implement LogisticRegression.grad using a for-loop. For an E, your solution should involve no explicit loops. While working on a solution that avoids loops, you might find it useful to at some point convert a tensor v with shape (n,) into a tensor v_ with shape (n,1). The code v_ = v[:, None] will perform this conversion for you."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#implement-gradientdescentoptimizer",
    "href": "posts/implementing-logistic-regression/index.html#implement-gradientdescentoptimizer",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "Implement GradientDescentOptimizer",
    "text": "Implement GradientDescentOptimizer\nNext, implement a GradientDescentOptimizer class. For this project, we are going to implement gradient descent with momentum, also known as Spicy Gradient Descent. Let wk be the estimate of the weight vector at algorithmic step k. Gradient descent with momentum performs the update equation."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#experimental-data",
    "href": "posts/implementing-logistic-regression/index.html#experimental-data",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "Experimental Data",
    "text": "Experimental Data\n\n# TESTING\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX_rand, y_rand = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#how-to-train-your-model",
    "href": "posts/implementing-logistic-regression/index.html#how-to-train-your-model",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "How to Train Your Model",
    "text": "How to Train Your Model\n\n# TRAIN the model\nfrom matplotlib import pyplot as plt\n\ndef train_and_plot(X, y, alpha, beta, iterations=100):\n\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    loss_vec = []\n\n    for _ in range(iterations):\n       # add other stuff to e.g. keep track of the loss over time. \n\n        loss = LR.loss(X, y)\n        loss_vec.append(loss)\n\n        opt.step(X, y, alpha, beta)\n\n    plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n    plt.semilogx()\n    labs = plt.gca().set(xlabel = \"Number of Gradient Descent iterations\", ylabel=\"loss\")\n\ntrain_and_plot(X_rand, y_rand, alpha=0.1, beta=0.9)"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#experiments",
    "href": "posts/implementing-logistic-regression/index.html#experiments",
    "title": "Implementing Logistic Regression [in progress]",
    "section": "Experiments",
    "text": "Experiments\nPlease perform experiments, with careful written explanations, that demonstrate the following statements:\n\n1. Vanilla gradient descent:\nWhen the number of features \\(p~dim~=2\\), when alpha is sufficiently small and beta=0, gradient descent for logistic regression converges to a weight vector w that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations). - This is a good experiment to use to assess whether your implementation in Part A has bugs.\n\nX_exp, y_exp = classification_data(n_points = 300, noise = 0.2, p_dims = 2)\n\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0)\n\n\n\n\n\n\n\n\n\n\n2. Benefits of momentum:\nOn the same data, gradient descent with momentum (e.g. beta = 0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta = 0). Plot the loss over iterations for each method. You may need to experiment with the data and choice of alpha in order to observe speedups due to momentum.\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0.9)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Implementing Logistic Regression [in progress]\n\n\n\n\n\n’Implementing Logistic Regression\n\n\n\n\n\nMay 12, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs? [in progress]\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nMar 7, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Blog\n\n\n\n\n\nPalmer Penguins!\n\n\n\n\n\nFeb 19, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\nNo matching items"
  }
]