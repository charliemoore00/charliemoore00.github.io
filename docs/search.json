[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Charlie, a CS student graduating this May from Middlebury College.\nThis website is dedicated to showing my learning in this semester’s Machine Learning course through a series of blog posts - these are short research projects aimed at polishing our skills working with data and constructing models, as well as thinking about the social and cultural impacts and bias present in the machine learning world."
  },
  {
    "objectID": "posts/penguins-blog-post/index.html",
    "href": "posts/penguins-blog-post/index.html",
    "title": "Penguins Blog",
    "section": "",
    "text": "The aim of this analysis is to explore and analyze the Palmer Penguins dataset and construct a classifier that can predict the species of penguin (Adelie, Gentoo, Chinstrap) based on the other factors in the data. The findings revealed that a logistic regression model performed the best in predicting the penguin species based on selected features. The model achieved a high accuracy score of 99.6% on the training data and 100% on the test data. We also found the determining features are culmen length, culmen depth, and island location that allow us to classify the penguin species.\nFirst, we’ll load the palmer penguins dataset:\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(url)\n\nWe’ll take a quick peek at how the data looks:\n\ntrain.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/penguins-blog-post/index.html#explore",
    "href": "posts/penguins-blog-post/index.html#explore",
    "title": "Penguins Blog",
    "section": "Explore",
    "text": "Explore\n\nCreating visualizations to learn about our dataset.\nThe following table shows the means, minimums, and maximums of each column in the dataset, grouped by island. One thing to point out is that the mean for “Stage_Adult, 1 Egg Stage” is 1.0, therefore it won’t help us predict the species. No need to include it in future code!\nAlso, the means for the Delta values are very similar across the islands, so we can guess that if we include island in our model, the delta values won’t be very helpful in predicting the species.\n\npd.set_option('display.max_columns', None)\nX_train.groupby([\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]).agg([\"mean\", \"min\", \"max\"])\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\nmean\nmin\nmax\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0\n1\n39.350000\n34.4\n46.0\n18.441176\n15.9\n21.5\n192.235294\n176.0\n210.0\n3727.941176\n2900.0\n4700.0\n8.844635\n7.69778\n9.59462\n-25.748135\n-26.53870\n-23.90309\n1.0\n1\n1\n0.205882\n0\n1\n0.794118\n0\n1\n0.470588\n0\n1\n0.529412\n0\n1\n\n\n1\n0\n44.527835\n34.0\n58.0\n18.307216\n16.4\n21.2\n193.628866\n178.0\n212.0\n3734.793814\n2700.0\n4800.0\n9.163175\n8.01485\n10.02544\n-25.075329\n-26.69543\n-23.89017\n1.0\n1\n1\n0.123711\n0\n1\n0.876289\n0\n1\n0.525773\n0\n1\n0.474227\n0\n1\n\n\n1\n0\n0\n44.945600\n34.5\n55.9\n15.863200\n13.1\n21.1\n209.320000\n172.0\n230.0\n4702.000000\n2850.0\n6300.0\n8.394096\n7.63220\n9.79532\n-26.086192\n-27.01854\n-24.36130\n1.0\n1\n1\n0.072000\n0\n1\n0.928000\n0\n1\n0.520000\n0\n1\n0.480000\n0\n1\n\n\n\n\n\n\n\n\n\nHeatmap\nThe following visualization shows the correlation between different pairs of variables. Values close to 1 or -1 indicate more linear relationships. Given that there are three penguin species, the most helpful visual is one that shows three distinct clumps. This would then likely not be a linear correlation, and so values that are closer to zero may show clumped data. We can see some of the lowest correlations are Culmen Depth vs Culmen Length, Body Mass vs Culmen Depth, and Body Mass vs Culmen Length.\n\ncorr = X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"]].corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\n\n\n\n\n\n\n\n\n\n\nScatterplot\nThe following scatterplot shows the relationship between Culmen Depth (mm) on the X-axis and Body Mass (g) on the Y-axis. The scatterplot is colored by the sex of the penguin, with purple representing female and orange representing male. This scatterplot shows two distinct clusters of penguins, one in the upper left and the other bottom right. Since we have color-coded by gender, we can see that the clusters are not separated by gender! Perhaps the are separated by Species, in which case this relationship will be very important when creating a classification model. It’s likely that one of the penguin species is represented by the cluster in the upper left, and the other two are represented by the cluster in the bottom right. I make this assumption given that the lower right cluster is roughly twice the size of the other.\n\nsns.scatterplot(X_train, x=\"Culmen Depth (mm)\", y=\"Body Mass (g)\", hue=\"Sex_MALE\", palette=\"CMRmap\")"
  },
  {
    "objectID": "posts/whose-costs/index.html",
    "href": "posts/whose-costs/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "Introduction\nThis blog post will analyze bank loan data, create an automated decision system for future prospective borrowers, and analyze how it impacts different demographics.\nWe explore the data and create a Logistic Regression model. Then we find an optimal threshold using some assumptions about the profit the bank makes on fully repaid versus defaulted loans. Then we test this new model on validation data and assess the ‘fairness’ of the model.\nWe found an optimal threshold that makes the bank $1386 per prospective borrower on validation data. We also find that in our new system, very young and very old people have a harder time accessing credit, it’s harder to receive loans for medical purposes compared to other intentions, and it’s easier to access credit when a prospective borrower has more income.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n\nPart A: Grab the Data\nWe’ll start by loading the data into a dataframe:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ntrain = pd.read_csv(url)\n\nWe’ll take a peak at how the data looks:\n\ntrain.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nPart B: Explore The Data\n\nSummary table\nThe following table shows the means of each quantitative column, grouped by loan_grade, descending from the best grade ‘A’, denoting confidence that the recipient will pay back the loan, to the worst grade ‘G’, denoting the least confidence that the recipient will pay back the loan. We can see that loan as percent of income increases as the loan grade decreases. Loan amount increases as the loan grade decreases. Loan interest rate increases as the loan grade decreases.\nInteresting: the lower grades have a higher income.\nI’ve selected just the columns with quantitative information and cleaned the data by removing rows with missing values.\n\nquant_col = [\"loan_grade\", \"loan_percent_income\", \"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"cb_person_cred_hist_length\"]\ntrain_quant = train[quant_col].dropna()\ntrain_quant.groupby([\"loan_grade\"]).aggregate(\"mean\") #, \"person_home_ownership\"\n\n\n\n\n\n\n\n\nloan_percent_income\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\ncb_person_cred_hist_length\n\n\nloan_grade\n\n\n\n\n\n\n\n\n\n\n\nA\n0.152629\n27.682077\n66773.007816\n5.122533\n8555.884885\n7.338368\n5.730560\n\n\nB\n0.173846\n27.673949\n66662.091096\n4.750069\n10031.025007\n11.003273\n5.766007\n\n\nC\n0.168928\n27.792667\n66416.633130\n4.402663\n9322.102794\n13.456237\n5.855303\n\n\nD\n0.188833\n27.853112\n64555.473908\n4.699652\n10821.646695\n15.358261\n5.861229\n\n\nE\n0.204190\n27.732955\n70868.349432\n4.458807\n12929.083807\n17.047344\n5.747159\n\n\nF\n0.220982\n28.564417\n80756.546012\n4.239264\n15395.705521\n18.519018\n6.214724\n\n\nG\n0.243409\n28.181818\n77342.477273\n5.954545\n17384.659091\n20.230000\n6.500000\n\n\n\n\n\n\n\nLet’s load our visualization library:\n\nimport seaborn as sns\n\n\n\nVisualization 1\nThis visualization shows the effect of loan grade on loan amount in two cases - if the person defaulted on their loan or not. We can see that as the loan amount increases, the loan grade decreases. This indicates loan amount may be a good predictor variable of loan status. While a default on file doesn’t have much effect on the loan amount, we can see that there are NO loans of grades A and B given to those with a default on file. This is an interesting bit of information that could definitely help predict loan status - on the other hand, it may be more of a mistrustful approach for the bank not wanting to lose money again, therefore skewing their loan grade variable.\n\nsns.boxplot(train, x=\"cb_person_default_on_file\", y=\"loan_amnt\", hue_order=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"], hue=\"loan_grade\", palette=\"Set2\", fliersize=0.5)\n\n\n\n\n\n\n\n\n\n\nVisualization 2\nHere we visualize a scatterplot where loan percent income is on the x-axis, and loan interest rate is on the y-axis. The color of the data corresponds to the loan grade given by the bank.\nWe can see that loan interest rate has a high correlation to loan grade - the higher the interest rate, the lower the loan grade.\n\norder_loan_grade = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\nsns.scatterplot(train, x=\"loan_percent_income\", y=\"loan_int_rate\", hue=\"loan_grade\", hue_order=order_loan_grade)\n\n\n\n\n\n\n\n\n\n\n\nPart C: Build a Model\nWe’re going to use cross validation to score all combinations of features to find the best ones to use in our model.\n\nData Preparation\nIn order to find the optimal variables for a model, we need to prep the data - specifically dropping rows with NAs and using one-hot encoding to turn the categorical variables into numerical ones.\n\ntrain = train.dropna()\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"loan_status\"])\n\ndef prepare_data(df):\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = 1*pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nIf we check how our data looks, we can see each categorical variable now is represented by multiple columns with binary values.\n\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_VENTURE\nloan_grade_A\nloan_grade_B\nloan_grade_C\nloan_grade_D\nloan_grade_E\nloan_grade_F\nloan_grade_G\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n5 rows × 26 columns\n\n\n\nWe will use the combinations function from the itertools package. This will list all the combinations of one discreet variable and two continuous variables. Iterating through all the possible combinations, it will A) score the model using cross validation, and B) return the best combination of columns that performed the best.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\n\nLR = LogisticRegression(max_iter=999)\n\nall_qual_cols = [\"person_home_ownership\", \"loan_intent\", \"cb_person_default_on_file\"]\nall_quant_cols = [\"person_age\", \"person_income\", \"person_emp_length\", \"loan_amnt\", \"loan_int_rate\", \"loan_percent_income\", \"cb_person_cred_hist_length\"]\n\nscore = 0\ncols_best = []\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n\n        #fit the models\n        log_score = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n        #compare the scores\n        if log_score &gt; score:\n            score = log_score\n            cols_best = cols\n\n#output the best score, along with the corresponding columns\nprint(score)\nprint(cols_best)\n\n0.8488229950993185\n['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_age', 'loan_percent_income']\n\n\nLooks like the best features to use are: person_home_ownership, person_age, and loan_percent_income\n\nLR = LogisticRegression(max_iter=999999999)\nmodel = LR.fit(X_train[cols_best], y_train)\n\nOur weights are now stored in:\n\nmodel.coef_\n\narray([[-7.52981385e-01, -8.75812948e-02, -1.79583155e+00,\n         2.81498971e-01, -4.00881551e-03,  8.27632650e+00]])\n\n\n\n\n\nPart D: Find a Threshold\nOnce we have a weight vector w, it is time to choose a threshold t. To choose a threshold that maximizes profit for the bank, we need to make some assumptions about how the bank makes and loses money on loans. Let’s use the following (simplified) modeling assumptions:\nIf the loan is repaid in full, the profit for the bank is equal to\nloan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt ( 1 )\nThis formula assumes that the profit earned by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going to things like salaries for the people who manage the bank. It is extremely simplistic and does not account for inflation, amortization over time, opportunity costs, etc.\nIf the borrower defaults on the loan, the “profit” for the bank is equal to\nloan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt ( 2 )\nThis formula corresponds to the same profit-earning mechanism as above, but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal.\n\nCalculate the Profit\nLet’s first create a function to calculate the profit from predicted target variables. It will take in the prediction column of 0s and 1s that guesses if a person will default or not, and return the total profit.\nFirst, we have to compare our predictions to the actual loan_status to then calculate the benefit or cost. There are three possible cases:\n\nIf we predict someone to default, regardless of what actually happened, we’ll treat it as a 0 because we wouldn’t give them a loan.\nIf we predict someone to not default - 0 - and that was actually the case - loan_status of 0, this is an instance of a true negative. We’ll use equation 1 to calculate our profit for a fully repaid loan.\nIf we predict someone to not default - 0 - and they actually did default - loan_status of 1, this is an instance of a false negative. We’ll use equation 2 to calculate our profit for a defaulted loan.\n\n\ndef profit(X, y, target_col):\n\n    num_people = y.size\n\n    # calculate new data - 0, 1, or nan given our target_col and loan_status\n    X[\"confusion\"] = np.where(target_col == 1, np.nan, np.where((target_col == 0) & (y == 0), 0, 1))\n    X.dropna()\n\n    # use our assumptions above to calculate overall sum\n    return ((X[\"loan_amnt\"] * (1 + 0.0025*X[\"loan_int_rate\"])**(10 - 7*X[\"confusion\"]) - (1 + 0.7*X[\"confusion\"])*X[\"loan_amnt\"]).sum())/num_people\n\nFirst, we’ll turn our scores into a numpy array w for ease\n\nw = np.array(model.coef_)[0]\nw\n\narray([-7.52981385e-01, -8.75812948e-02, -1.79583155e+00,  2.81498971e-01,\n       -4.00881551e-03,  8.27632650e+00])\n\n\nLet’s create a function linear_score that computes the weighted score using our 6 predictor columns and weights w\n\ndef linear_score(X, w):\n    return X@w\n\nLet’s also create a predict method to guess based off of our threshold:\n\ndef predict(w, threshold, df):\n    \n    # compute the scores using weights and predictor variables\n    scores = linear_score(df, w)\n\n    # scipy's minimize function messes with the var type, so we'll fix it here\n    if isinstance(threshold, np.ndarray):\n        threshold = threshold.item()\n\n    # the actual thresholding step that generates binary predictions\n    return 1*(scores&gt;threshold)\n\nWe’ll use this function to add a new column into our dataframe of our predictions based off of our threshold. Let’s start off with a guess of 1 as the threshold:\n\nX_train[\"prediction\"] = predict(w, 1, X_train[cols_best])\n\nLet’s see how well our guess does!\n\n(X_train[\"prediction\"] == y_train).mean()\n\n0.6271881957480246\n\n\nThat reflects how accurate we were - but how about finding the profit from that threshold? That’s ultimately the goal here.\n\nprofit(X_train, y_train, X_train[\"prediction\"])\n\n869.6881282011279\n\n\nNice! Looks like this would result in a profit per person of $870! But we can do better - now let’s calculate the optimal threshold that allows our bank to make the most profit.\nLet’s create a master function optimal that takes in a threshold t and returns the profit\n\ndef optimal(t):\n    return profit(X_train, y_train, predict(w, t, X_train[cols_best]))\n\nNow let’s graph this function over the span of possible thresholds\n\nt_values = np.linspace(-6, 8, 100)\noptimal_values = [optimal(t) for t in t_values]\n\n# Plotting the function\nplt.figure(figsize=(10, 5))\nplt.plot(t_values, optimal_values, label='Optimal(t)', color='blue')\nplt.title('Graph of Optimal(t) from -8 to 8')\nplt.xlabel('t')\nplt.ylabel('Optimal(t)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLooks like there’s a clear maximum for t somewhere in the 2 - 4 range\nNow we’ll use scipy’s minimize method to find the optimal value for the threshold.\n\nfrom scipy.optimize import minimize\n\ninit_threshold = 1\nresult = minimize(lambda t: -optimal(t), x0 = init_threshold, method='Nelder-Mead')\n\noptimal_threshold = result.x[0]\nprint(\"Optimal threshold: \", optimal_threshold)\nprint(\"Maximum Profit: \", optimal(optimal_threshold))\n\n\nOptimal threshold:  2.6687500000000033\nMaximum Profit:  1445.9819108569352\n\n\nWe’ve now found our optimal threshold t of approximately 2.69, resulting in a profit for the bank of about $1446 per person.\n\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\nNow that we have the weight vector w and optimal threshold t, let’s evaluate the automated decision-process on a test set:\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ntest = pd.read_csv(url)\n\ntest.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n21\n42000\nRENT\n5.0\nVENTURE\nD\n1000\n15.58\n1\n0.02\nN\n4\n\n\n1\n32\n51000\nMORTGAGE\n2.0\nDEBTCONSOLIDATION\nB\n15000\n11.36\n0\n0.29\nN\n9\n\n\n2\n35\n54084\nRENT\n2.0\nDEBTCONSOLIDATION\nC\n3000\n12.61\n0\n0.06\nN\n6\n\n\n3\n28\n66300\nMORTGAGE\n11.0\nMEDICAL\nD\n12000\n14.11\n1\n0.15\nN\n6\n\n\n4\n22\n70550\nRENT\n0.0\nMEDICAL\nE\n7000\n15.88\n1\n0.08\nN\n3\n\n\n\n\n\n\n\nLet’s start by filtering to just our predictor columns and preparing the data in the same way as the train set:\n\ntest = test.dropna()\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(test[\"loan_status\"])\n\ndef prepare_data(df):\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = 1*pd.get_dummies(df)\n  return df, y\n\nX_test, y_test = prepare_data(test)\n\n\nX_test[\"prediction\"] = predict(w, optimal_threshold, X_test[cols_best])\nprofit(test, y_test, X_test[\"prediction\"])\n\n1386.0022776563628\n\n\nThe test set yielded a profit of $1386 per person. Very close to our training numbers!\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\nNow let’s evaluate the model from the perspective of the prospective borrowers by answering some questions about how different demographics have different access to credit.\n\n1. Is it more difficult for people in certain age groups to access credit under the proposed system?\nTo answer this, let’s first create a new dataframe to plot age and our model’s predictions.\n\nage = test[[\"person_age\"]]\nage[\"prediction\"] = X_test[\"prediction\"]\n\n# we'll create bins for age ranges\nbins = [20, 25, 30, 35, 40, 45, 50, 55, 60, 100]\nlabels = ['20-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60+']\n\n# categorize person_age into age groups\nage['age_group'] = pd.cut(test['person_age'], bins=bins, labels=labels)\n\nThen we’ll plot a bar chart with age group on the x-axis and average prediction from 0-1 on the y-axis, showing how likely it is that someone in that age group will receive a loan from the bank. The lower the average prediction, the more likely they are to receive a loan.\n\n# calculate means for each group\ngroup_means = age.groupby('age_group')['prediction'].mean().reset_index()\n\n# create a bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(x='age_group', y='prediction', data=group_means)\nplt.title('Average Prediction by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Average Prediction')\nplt.show()\n\n/var/folders/rd/vvrhhjq53ds_4zcgxzqhtlc40000gn/T/ipykernel_88614/452989469.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = age.groupby('age_group')['prediction'].mean().reset_index()\n\n\n\n\n\n\n\n\n\nFrom this information, there is a clear trend where from you 20s, as you get older you are more likely to receive a loan - then after 40, you become less likely to receive a loan as you get older. In other words, the closer you are to the 35-49 range, the more likely you’ll be to receive a loan. This makes sense, as that’s the age people are looking for homes and cars and big purchases, while also being in the midst of a career with plenty of time to pay off the loan. Younger groups have less financial stability, and older groups have less time and income.\n\n\n2. Is it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nTo answer this question, we’ll do a similar approach to the last problem where we see that predicted average default, and compare it to the actual default.\n\nintent = test[[\"loan_intent\", \"loan_status\"]]\nintent[\"prediction\"] = X_test[\"prediction\"]\n\n\npred_means = intent.groupby('loan_intent')['prediction'].mean().reset_index()\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x='loan_intent', y='prediction', data=pred_means)\nplt.title('Histogram of Predictions Grouped by Loan Intent')\nplt.xlabel('Loan Intent')\nplt.ylabel('Average Prediction')\nplt.show()\n\n\n\n\n\n\n\n\nUnder this new system, those seeking a loan for medical reasons are less likely to receive a loan than most other intentions like education and business ventures.\n\nactual_means = intent.groupby('loan_intent')['loan_status'].mean().reset_index()\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x='loan_intent', y='loan_status', data=actual_means)\nplt.title('Histogram of Loan Status Grouped by Loan Intent')\nplt.xlabel('Loan Intent')\nplt.ylabel('Average Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\nBased on the actual rate of default as shown just above, people seeking a loan for medical reasons will default more than most other groups like education and venture.\nBased on the past 2 visuals, this new system will predict people to default proportionally to actual rates of default when looking at loan intent.\n\n\n3. How does a person’s income level impact the ease with which they can access credit under your decision system?\nLet’s plot income level against default predictions:\n\nincome = test[[\"person_income\"]]\nincome[\"prediction\"] = X_test[\"prediction\"]\n\n# we'll create bins for income ranges\nbins = [0, 10000, 30000, 50000, 70000, 90000, 120000, 200000, 99999999]\nlabels = ['0-10K', '10K-30K', '30K-50K', '50K-70K', '70K-90K', '90K-120K', '120K-200K', '200K+']\n\n# categorize person_income into bins\nincome['person_income'] = pd.cut(test['person_income'], bins=bins, labels=labels)\n\n\n# calculate means for each group\ngroup_means = income.groupby('person_income')['prediction'].mean().reset_index()\n\n# create a bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(x='person_income', y='prediction', data=group_means)\nplt.title('Average Prediction by Income')\nplt.xlabel('Income ($)')\nplt.ylabel('Average Prediction')\nplt.show()\n\n/var/folders/rd/vvrhhjq53ds_4zcgxzqhtlc40000gn/T/ipykernel_88614/3325855078.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = income.groupby('person_income')['prediction'].mean().reset_index()\n\n\n\n\n\n\n\n\n\nThis visual shows that the more income a prospective borrower has, the more likely they are to be given a loan under this system.\n\n\n\nConclusion\nBy creating a Logistic Regression model, we were able to find the vector of weights w. We then used the weight vector to find an optimal threshold t. We used some basic assumptions to model a bank’s profit given the scenarios of fully paying off a loan or defaulting, and these let us find a threshold that maximized the total profit for the bank.\nAfter finding a model, we wanted to assess it not only on the bank’s perspective, but also on the borrower’s perspective. We found that some demographics are more likely to be given a loan than others. Younger borrowers and older borrowers are less likely to have access to credit than those around the ages of 35 - 49. People trying to receive loans for medical and debt reasons are much less likely to receive a loan than if the purpose is for education. Lastly, in this new model borrowers with less income are less likely to receive loans - the more income the borrower has, the more likely they’ll have access to credit.\nConcerning that people seeking loans for medical expenses have high rates of default, is it fair that it is more difficult for them to obtain access to credit? To answer this, we’re going to use the narrow view of fairness - that ‘similar individuals should be treated similarly’. In the real world people seeking a loan for medical bills are more likely to default on that loan than people looking for a loan for education. This is reflected similarly in the model we constructed. This model holds the narrow view, and so from the bank’s perspective it’s a fair model. Furthermore, the bank’s only incentive is to maximize profit, regardless of any societal bias that exists outside of giving loans. Thus, the bank has no obligation to change it’s behavior.\nI would also add that if specific demographics of people have a hard time getting loans for medical bills, than it points to a larger systemic issue. This is a simplistic model, but it signifies a larger lesson that it is hard to hold the private sector accountable - when inequality is fed in to it, unequal outcomes will be spit back out."
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Newton’s Method for Logistic Regression is implemented in newton.py"
  },
  {
    "objectID": "posts/newtons-method/index.html#abstract",
    "href": "posts/newtons-method/index.html#abstract",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nThis blog post contains an implementation of Newton’s Method for Logistic Regression, located in the newton.py class available above. This is an alternative optimizer to Gradient Descent. This blog post will conduct a few experiments about how Newton’s Method compares to Gradient Descent in terms of the speed it takes to converge, and also the computation cost.\n\n%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt # type: ignore\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/newtons-method/index.html#part-a-implement-newtonoptimizer",
    "href": "posts/newtons-method/index.html#part-a-implement-newtonoptimizer",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part A: Implement NewtonOptimizer",
    "text": "Part A: Implement NewtonOptimizer\nNewton’s Method is a second-order optimization technique. This means that it requires information about the second derivatives of the loss function as well as the first derivatives. Here’s how Newton’s method works:\n\nWe compute the usual gradient \\(\\nabla L(\\mathbf{w})\\), which is the vector of first derivatives of \\(L\\).\nWe also compute the Hessian Matrix, which is the matrix of second derivatives of \\(L\\). For logistic regression, the Hessian is the matrix \\(\\mathbf{H}(\\mathbf{w}) \\in \\mathbb{R}^{p \\times p}\\) with entries\n\\[\\begin{aligned}\nh_{ij}(\\mathbf{w}) = \\sum_{k = 1}^n x_{ki}x_{kj}\\sigma(s_k)(1-\\sigma(s_k))\\;\n\\end{aligned}\\]\nTo compute this matrix, we’ll use the formula \\(\\mathbf{H}(\\mathbf{w}) = \\mathbf{X}^T\\mathbf{D}(\\mathbf{w})\\mathbf{X}\\) where \\(D\\) is the diagonal matrix with entries \\(d_{kk}(\\mathbf{w}) = \\sigma(s_k)(1-\\sigma(s_k))\\).\nOnce we know how to calculate the gradient and the Hessian, we repeat the update\n\\[\\begin{aligned}\nw \\gets w - \\alpha \\mathbf{H}(\\mathbf{w})^{-1} \\nabla L (\\mathbf{w})\\;\n\\end{aligned}\\]\nuntil convergence. Here, \\(\\alpha &gt; 0\\) is the learning rate and \\(\\mathbf{H}(\\mathbf{w})^{-1}\\) is the matrix inverse of the Hessian Matrix.\n\nThe newton.py file linked at the top implements a NewtonOptimizer class that uses Newton’s method to estimate w for a LogisticRegression model."
  },
  {
    "objectID": "posts/newtons-method/index.html#part-b-experimenting",
    "href": "posts/newtons-method/index.html#part-b-experimenting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part B: Experimenting",
    "text": "Part B: Experimenting\nLet’s conduct a few experiments to show the functionality of Newton’s Method, and how it compares to Gradient Descent.\n\nWhen \\(\\alpha\\) is chosen appropriately, Newton’s method converges to the correct choice of w.\nUnder at least some circumstances, Newton’s method can converge much faster than standard gradient descent, in the sense of decreasing the empirical risk.\nIf \\(\\alpha\\) is too large, Newton’s method fails to converge.\n\n\nGenerating Experimental Data\nHere is a method to generate data. The parameters are:\n\nn_points: the number of points\np_dims: the number of features\nnoise: the difficulty of the classification problem\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX_rand, y_rand = classification_data(noise = 0.5)\n\n\n\nTraining the Model\nWe’ll create train and plot methods for both Newton’s method and regular Gradient Descent to show differences.\n\nGradient Descent Training\n\ndef train_and_plot_gradient_descent(X, y, alpha, beta, iterations=100):\n    # initialize a logistic regression and gradient descent optimizer\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n    # to keep track of the loss to plot it\n    loss_vec = []\n\n    for _ in range(iterations):\n\n       # keep track of the loss over time. \n        loss = LR.loss(X, y)\n        loss_vec.append(loss)\n        # use GradientDescentOptimizer's step function\n        opt.step(X, y, alpha, beta)\n\n    # plot the loss\n    plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n    plt.semilogx()\n    labs = plt.gca().set(xlabel = \"Number of Gradient Descent iterations\", ylabel=\"loss\")\n\n\n\nNewton’s Method Training\n\ndef train_and_plot_newton_optimizer(X, y, alpha, iterations=100):\n    # initialize a logistic regression and gradient descent optimizer\n    LR = LogisticRegression() \n    opt = NewtonOptimizer(LR)\n    # to keep track of the loss to plot it\n    loss_vec = []\n\n    for _ in range(iterations):\n       # keep track of the loss over time. \n        loss = LR.loss(X, y)\n        loss_vec.append(loss)\n        # use NewtonOptimizer's step function\n        opt.step(X, y, alpha)\n\n    # plot the loss\n    plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n    plt.semilogx()\n    labs = plt.gca().set(xlabel = \"Number of Newton Optimizer iterations\", ylabel=\"loss\")\n\n\n\nExperiment #1: When \\(\\alpha\\) is chosen appropriately, Newton’s method converges to the correct choice of w.\nFirst, let’s generate some data:\n\nX_exp, y_exp = classification_data(n_points = 3000, noise = 0.5, p_dims = 2)\n\nThen we’ll train and plot our data with an \\(\\alpha\\) value of 200:\n\ntrain_and_plot_newton_optimizer(X_exp, y_exp, 200, iterations = 100)\n\n\n\n\n\n\n\n\nWe can see of Logistic Regression with a Newton Optimizer does indeed converge within 100 iterations!\n\n\nExperiment #2: Under at least some circumstances, Newton’s method can converge much faster than standard gradient descent\nLet’s train and plot both gradient descent and Newton’s method on the same data as before, but on 1000 iterations:\n\n# gradient descent\ntrain_and_plot_gradient_descent(X_exp, y_exp, 0.1, 0.9, iterations = 1000)\n\n\n\n\n\n\n\n\n\n# Newton's method\ntrain_and_plot_newton_optimizer(X_exp, y_exp, 2000, iterations=1000)\n\n\n\n\n\n\n\n\nWe can see that while it takes gradient descent between 100-1000 iterations to converge, it takes Newton’s method less than 10. This is over a 10x speedup! But keep in mind this data has a low number of features.\n\n\nExperiment #3: If \\(\\alpha\\) is too large, Newton’s method fails to converge.\nLet’s set the alpha to a really high number and see what happens, keeping the same parameters as before:\n\ntrain_and_plot_newton_optimizer(X_exp, y_exp, 9999, iterations=1000)\n\nRuntimeError: torch.linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\n\n\nWe end up getting the following error: &gt; RuntimeError: torch.linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\nSince our alpha value was so large, when we ran the NewtonOptimizer.step method, the updates to the weights were so large that it diverged instead of converged."
  },
  {
    "objectID": "posts/newtons-method/index.html#part-c-operation-counting",
    "href": "posts/newtons-method/index.html#part-c-operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part C: Operation Counting",
    "text": "Part C: Operation Counting\nIn high-dimensional optimization, the number of features p can be very large. This can be a problem for Newton’s method, because the operation of inverting a \\(p\\times p\\) matrix requires \\(O(p^\\gamma)\\) operations for some \\(2 \\leq \\gamma &lt;3\\). Multiplying the gradient by the Hessian also requires O(p^2) operations.\nAssume that it costs c computational units to compute the loss \\(L\\), \\(2c\\) units to compute the gradient \\(\\nabla L\\), and \\(pc\\) units to compute the Hessian. Suppose further that it costs \\(k_1p^\\gamma\\) units to invert a \\(p\\times p\\) matrix and \\(k_2p^{2}\\) units to perform the matrix-vector multiplication required by Newton’s method.\nFinally, suppose that Newton’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\) steps, while gradient descent converges to an adequate solution in \\(t_\\mathrm{gd}\\) steps.\nUnder these assumptions, write expressions describing the total computational costs of Newton’s method as compared to gradient descent. How much smaller must \\(t_\\mathrm{nm}\\) be than \\(t_\\mathrm{gd}\\) in order to ensure that Newton’s method will require fewer computational units to complete? When p becomes very large, is using Newton’s method ever going to pay off?\nTo answer this, let’s break down the computational cost of Newton’s Method and Gradient Descent:\n\nNewton’s Method:\nCost of computation for: - Loss \\(L\\): \\(c\\) - Gradient \\(\\nabla L\\): \\(2c\\) - Hessian matrix: \\(pc\\) - Inverting Hessian matrix: \\(k_1p^\\gamma\\) - matrix-vector multiplication: \\(k_2p^{2}\\) Total iterations: \\(t_\\mathrm{nm}\\)\nWe can consolidate this into the expression: \\(Newton's = t_\\mathrm{nm} * (3c + pc + k_1p^\\gamma + k_2p^{2})\\)\n\n\nGradient Descent\nCost of computation for: - Loss \\(L\\): \\(c\\) - Gradient \\(\\nabla L\\): \\(2c\\) Total iterations: \\(t_\\mathrm{gd}\\)\nWe can consolidate this into the expression: \\(GradientD = t_\\mathrm{gd} * (2c)\\)\nWe are looking for \\(Newton's &lt; GradientD\\). We are looking for big Oh values here, so we can just take into account the largest terms - in this case, \\(k_1p^\\gamma + k_2p^{2}\\). This means that the number of iterations for Newton’s Method \\(t_\\mathrm{nm}\\) must be around the order of \\(p^{2}\\) times smaller than \\(t_\\mathrm{gd}\\) to converge at less computational cost.\nWhen p becomes vey large, \\(p^{2}\\) becomes extremely large and Newton’s Method will be less and less likely to pay off. Thus Newton’s Method is better for low-dimensional problems."
  },
  {
    "objectID": "posts/deep-music-genre-classification/index.html",
    "href": "posts/deep-music-genre-classification/index.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "First we’ll import our libraries and data for this blog post.\n\nimport torch\nimport numpy as np\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n/Users/charliemoore/anaconda3/envs/ml-0451/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nI accessed the data on Kaggle here. The data was originally collected from Spotify by researchers who published in the following data publication:\n\nMoura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; França, Mardônio (2020), “Music Dataset: Lyrics and Metadata from 1950 to 2019”, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3\n\nHere’s an excerpt of the data:\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\nWe’re going to use Torch to predict the genre of the track based on the track’s lyrics and engineered features. The lyrics are contained in the lyrics column.\nIt will also be useful to have a list of the engineered features:\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']      \n\nThe features were engineered by teams at Spotify to describe attributes of the tracks.\nLet’s see what are base classification rate is:\n\ntotal = len(df)\ndf.groupby([\"genre\"]).size() / total\n\ngenre\nblues      0.162273\ncountry    0.191915\nhip hop    0.031862\njazz       0.135521\npop        0.248202\nreggae     0.088045\nrock       0.142182\ndtype: float64\n\n\nLooks like the most popular genre is pop at ~25%. Let’s construct some models to try and do better!"
  },
  {
    "objectID": "posts/deep-music-genre-classification/index.html#first-model-only-lyrics",
    "href": "posts/deep-music-genre-classification/index.html#first-model-only-lyrics",
    "title": "Deep Music Genre Classification",
    "section": "First Model: Only Lyrics",
    "text": "First Model: Only Lyrics\nTo use text to predict the genre, we’ll use word embeddings.\n\n# for embedding visualization later:\nimport plotly.express as px\nimport plotly.io as pio\n\n# for VSCode plotly rendering\npio.renderers.default = \"notebook\"\n\n# for appearance\npio.templates.default = \"plotly_white\"\n\n# for train-test split\nfrom sklearn.model_selection import train_test_split\n\nWe’re now going to encode the genres as integers:\n\ngenres = {\n    \"blues\"     : 0,\n    \"country\"   : 1,\n    \"hip hop\"   : 2,\n    \"jazz\"      : 3,\n    \"pop\"       : 4,\n    \"reggae\"    : 5,\n    \"rock\"      : 6\n}\n\ndf_lyrics = df[[\"genre\", \"lyrics\"]]\ndf_lyrics = df_lyrics[df_lyrics[\"genre\"].apply(lambda x: x in genres.keys())]\ndf_lyrics.head()\n\n\n\n\n\n\n\n\ngenre\nlyrics\n\n\n\n\n0\npop\nhold time feel break feel untrue convince spea...\n\n\n1\npop\nbelieve drop rain fall grow believe darkest ni...\n\n\n2\npop\nsweetheart send letter goodbye secret feel bet...\n\n\n3\npop\nkiss lips want stroll charm mambo chacha merin...\n\n\n4\npop\ntill darling till matter know till dream live ...\n\n\n\n\n\n\n\n\ndf_lyrics[\"genre\"] = df_lyrics[\"genre\"].apply(genres.get)\ndf_lyrics.head()\n\n\n\n\n\n\n\n\ngenre\nlyrics\n\n\n\n\n0\n4\nhold time feel break feel untrue convince spea...\n\n\n1\n4\nbelieve drop rain fall grow believe darkest ni...\n\n\n2\n4\nsweetheart send letter goodbye secret feel bet...\n\n\n3\n4\nkiss lips want stroll charm mambo chacha merin...\n\n\n4\n4\ntill darling till matter know till dream live ...\n\n\n\n\n\n\n\nWe now need to wrap the Pandas dataframe as a Torch dataset.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n# create our custom data loader class\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __getitem__(self, index):\n        # returns an item (row) of the dataset as the words then the label\n        return self.df.iloc[index, 1], self.df.iloc[index, 0]\n    \n    def __len__(self):\n        return len(self.df)\n\nNow let’s perform a train-validation split and make Datasets from each one.\n\ndf_train, df_val = train_test_split(df_lyrics, shuffle=True, test_size=0.2)\nlyrics_train_data   = TextDataFromDF(df_train)\nlyrics_val_data     = TextDataFromDF(df_val)\n\nLet’s take a look at one element of our train set:\n\nlyrics_train_data[68]\n\n('go girl dream school go girl fool ring curtain certain present future pass know speak tie tie break life wayto break future pass star blue distance encounter resistance help miss arm illusion look heart confusion love live future pass',\n 3)\n\n\n\nText Vectorization\nNow we’ll vectorize our text using a tokenizer to split sentences into individual words.\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(lyrics_train_data[68][0])\ntokenized\n\n['go',\n 'girl',\n 'dream',\n 'school',\n 'go',\n 'girl',\n 'fool',\n 'ring',\n 'curtain',\n 'certain',\n 'present',\n 'future',\n 'pass',\n 'know',\n 'speak',\n 'tie',\n 'tie',\n 'break',\n 'life',\n 'wayto',\n 'break',\n 'future',\n 'pass',\n 'star',\n 'blue',\n 'distance',\n 'encounter',\n 'resistance',\n 'help',\n 'miss',\n 'arm',\n 'illusion',\n 'look',\n 'heart',\n 'confusion',\n 'love',\n 'live',\n 'future',\n 'pass']\n\n\nNow we’ll start constructing a vocabulary - a mapping from words to integers.\n\ndef yield_tokens(data_iter):\n    for text, _ in data_iter:\n        yield tokenizer(text)\n\n# since there are so many words in this set, we'll use only those that appear at least 50 times using min_freq\nvocab = build_vocab_from_iterator(yield_tokens(lyrics_train_data), specials=[\"&lt;unk&gt;\"], min_freq = 50)\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\nHere are the first 10 elements from the vocabulary:\n\nvocab.get_itos()[0:10]\n\n['&lt;unk&gt;',\n 'know',\n 'like',\n 'time',\n 'come',\n 'go',\n 'away',\n 'heart',\n 'feel',\n 'yeah']\n\n\nWe can apply it on a list of tokens:\n\nvocab(tokenized)\n\n[5,\n 46,\n 31,\n 395,\n 5,\n 46,\n 98,\n 195,\n 1480,\n 790,\n 1423,\n 331,\n 186,\n 1,\n 197,\n 651,\n 651,\n 24,\n 10,\n 0,\n 24,\n 331,\n 186,\n 225,\n 55,\n 910,\n 0,\n 2869,\n 138,\n 110,\n 129,\n 1444,\n 26,\n 7,\n 1184,\n 62,\n 15,\n 331,\n 186]\n\n\n\n\nBatch Collation\nNow we’re ready to construct the function that is going to actually pass a batch of data to our training loop. Here are the main steps:\n\nWe pull some feature data (i.e. a batch of lyrics).\nWe represent lyrics as a sequence of integers using the vocab.\nWe pad the lyrics with an unused integer index if necessary so that all lyrics have the same length. This index corresponds to “blank” or “no words in this slot.”\nWe return the batch of lyrics as a consolidated tensor.\n\n\nmax_len = 500\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\n\ntext_pipeline(\"we can't believe\")\n\ntensor([   0,    0,    0,    0,   42, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875,\n        2875, 2875, 2875, 2875, 2875, 2875, 2875, 2875])\n\n\n\ndef collate_batch(batch):\n    label_list, text_list = [], []\n    for (_text, _label) in batch:\n\n        # add label to list\n         label_list.append(label_pipeline(_label))\n\n         # add text (as sequence of integers) to list\n         processed_text = text_pipeline(_text)\n         text_list.append(processed_text)\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    text_list = torch.stack(text_list)\n    return text_list, label_list\n\n\ntrain_loader = DataLoader(lyrics_train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(lyrics_val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n\nLet’s take a look at a batch of data now:\n\nnext(iter(train_loader))\n\n(tensor([[  54,   30,    0,  ..., 2875, 2875, 2875],\n         [ 604,  158, 2567,  ..., 2875, 2875, 2875],\n         [  19,  539,   48,  ..., 2875, 2875, 2875],\n         ...,\n         [ 803, 2327,  734,  ..., 2875, 2875, 2875],\n         [  23,  315,    7,  ..., 2875, 2875, 2875],\n         [   0,   60,   36,  ..., 2875, 2875, 2875]]),\n tensor([4, 4, 0, 1, 1, 3, 0, 6]))"
  },
  {
    "objectID": "posts/deep-music-genre-classification/index.html#modeling",
    "href": "posts/deep-music-genre-classification/index.html#modeling",
    "title": "Deep Music Genre Classification",
    "section": "Modeling",
    "text": "Modeling\n\nWord Embedding\nA word embedding refers to a representation of a word in a vector space. Each word is assigned an individual vector. The general aim of a word embedding is to create a representation such that words with related meanings are close to each other in a vector space, while words with different meanings are farther apart.\nLet’s learn and train a model!\n\nimport time\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\ndef train(dataloader, model):\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n\n    for idx, (text, label) in enumerate(dataloader):\n        # zero gradients\n        optimizer.zero_grad()\n        # form prediction on batch\n        predicted_label = model(text)\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        \n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n    \ndef evaluate(dataloader, model):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (text, label) in enumerate(dataloader):\n            predicted_label = model(text)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count\n\nFirst Model\n\nfrom torch import nn\n\nclass TextClassificationByLyrics(nn.Module):\n    \n    def __init__(self,vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.fc   = nn.Linear(max_len*embedding_dim, num_class)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return(x)\n\n\nvocab_size = len(vocab)\nembedding_dim = 3\nlyrics_model = TextClassificationByLyrics(vocab_size, embedding_dim, max_len, 7)\n\n\nEPOCHS = 20\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, lyrics_model)\n\n| epoch   1 | train accuracy    0.177 | time:  7.56s\n| epoch   2 | train accuracy    0.183 | time:  7.51s\n| epoch   3 | train accuracy    0.189 | time:  7.34s\n| epoch   4 | train accuracy    0.185 | time:  7.34s\n| epoch   5 | train accuracy    0.188 | time:  7.58s\n| epoch   6 | train accuracy    0.192 | time:  7.38s\n| epoch   7 | train accuracy    0.196 | time:  7.47s\n| epoch   8 | train accuracy    0.197 | time:  7.51s\n| epoch   9 | train accuracy    0.192 | time:  7.37s\n| epoch  10 | train accuracy    0.196 | time:  7.39s\n| epoch  11 | train accuracy    0.193 | time:  7.37s\n| epoch  12 | train accuracy    0.199 | time:  7.37s\n| epoch  13 | train accuracy    0.194 | time:  7.37s\n| epoch  14 | train accuracy    0.197 | time:  7.36s\n| epoch  15 | train accuracy    0.195 | time:  7.49s\n| epoch  16 | train accuracy    0.197 | time:  7.35s\n| epoch  17 | train accuracy    0.190 | time:  7.46s\n| epoch  18 | train accuracy    0.195 | time:  7.48s\n| epoch  19 | train accuracy    0.199 | time: 10.13s\n| epoch  20 | train accuracy    0.199 | time:  7.96s\n\n\n\nevaluate(val_loader, lyrics_model)\n\n0.186784140969163"
  },
  {
    "objectID": "posts/deep-music-genre-classification/index.html#second-model-only-engineered-features",
    "href": "posts/deep-music-genre-classification/index.html#second-model-only-engineered-features",
    "title": "Deep Music Genre Classification",
    "section": "Second Model: Only Engineered Features",
    "text": "Second Model: Only Engineered Features"
  },
  {
    "objectID": "posts/deep-music-genre-classification/index.html#third-model-lyrics-engineered-features",
    "href": "posts/deep-music-genre-classification/index.html#third-model-lyrics-engineered-features",
    "title": "Deep Music Genre Classification",
    "section": "Third Model: Lyrics + Engineered Features",
    "text": "Third Model: Lyrics + Engineered Features"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html",
    "href": "posts/implementing-logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Gradient Descent for Logistic Regression is implemented in logistic.py"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#abstract",
    "href": "posts/implementing-logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nThis blog post contains an implementation of Logistic Regression with a Gradient Descent with Momentum Optimizer. The implementation exists as the logistic.py class, available above. We’ll set up some sample data, train a model, and conduct some experiments using this implementation:\n\nClassic Gradient Descent without momentum: using just two features\nGradient Descent with momentum: how does momentum affect how quickly and accurately our model converges?\nOverfitting: how does the relationship between number of data points and number of features affect our model’s accuracy?\n\nFirst, let’s load our logistic regression implementation and other necessary packages.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt # type: ignore"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#implementing-logistic-regression",
    "href": "posts/implementing-logistic-regression/index.html#implementing-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Implementing Logistic Regression",
    "text": "Implementing Logistic Regression\nThe following classes are implemented in logistic.py - the link is at the top of this page.\n\nImplement LinearModel, LogisticRegression(), and GradientDescentOptimizer\nThe LogisticRegression() class has two methods:\n\nLogisticRegression.loss(X, y) computes the empirical risk \\(L(w)\\) using the logistic loss function:\n\\[\\begin{aligned}\nL(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right]\n\\end{aligned}\\]\n.\nLogisticRegression.grad(X, y) computes the gradient of the empirical risk \\({\\nabla}L(w)\\) using this formula:\n\\[\\begin{aligned}\n\\nabla L(\\mathbf{w}) &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i\\rangle) - y_i)\\mathbf{x}_i\\;\n\\end{aligned}\\]\n.\n\nThe GradientDescentOptimizer class has one method:\n\nGradientDescentOptimizer.step(X, y, alpha, beta) updates the weight vector. At algorithmic step k, it computes new weights:\n\\[\\begin{aligned}\n  \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\\]\nHere, alpha and beta are two learning rate parameters. When beta = 0 we have “regular” gradient descent. In practice, a choice of beta = 0.9 is common."
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#experiments",
    "href": "posts/implementing-logistic-regression/index.html#experiments",
    "title": "Implementing Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nGenerating Experimental Data\nHere is a method to generate data. The parameters are:\n\nn_points: the number of points\np_dims: the number of features\nnoise: the difficulty of the classification problem\n\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX_rand, y_rand = classification_data(noise = 0.5)\n\n\n\nTraining the Model\nWe’ll create a function that trains the model by iterating over the optimizer’s ‘step’ function.\n\ndef train_and_plot(X, y, alpha, beta, iterations=100):\n\n    # initialize a ligistic regression and optimizer\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n\n    # to keep track of the loss to plot it\n    loss_vec = []\n\n    for _ in range(iterations):\n\n       # keep track of the loss over time. \n        loss = LR.loss(X, y)\n        loss_vec.append(loss)\n\n        # use GradientDescentOptimizer's step function\n        opt.step(X, y, alpha, beta)\n\n\n    # plot the loss\n    plt.plot(torch.arange(1, len(loss_vec) +1), loss_vec, color=\"black\")\n    plt.semilogx()\n    labs = plt.gca().set(xlabel = \"Number of Gradient Descent iterations\", ylabel=\"loss\")"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#conduct-some-experiments",
    "href": "posts/implementing-logistic-regression/index.html#conduct-some-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Conduct Some Experiments!",
    "text": "Conduct Some Experiments!\n\n1. Vanilla gradient descent:\nWhen the number of features p_dim = 2, alpha is sufficiently small, and beta=0, gradient descent for logistic regression converges to a weight vector w that looks visually correct and the loss decreases monotonically.\nFirst, lets generate some experimental data with 2 dimensions:\n\nX_exp, y_exp = classification_data(n_points = 300, noise = 0.5, p_dims = 2)\n\nNext, let’s run our train_and_plot method with a sufficiently small value (0.1) for alpha, and with beta = 0.\nThis is a basic version of gradient descent without momentum (the beta term).\n\nHaving a small alpha value, or learning rate, means that our gradient will converge smoothly without any big jumps or overshooting the minimum.\nHaving a beta value of 0 means that the last term in the GradientDescentOptimizer.step function will evaluate to 0. This means it won’t take into account the old weight vector and add momentum to the term when updating the new weights.\n\nWith these parameters, we should see a smooth curve with the loss constantly decreasing as our step function iterates:\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0, iterations = 10000)\n\n\n\n\n\n\n\n\n\n\n2. Benefits of momentum:\nOn the same data, gradient descent with momentum (e.g. beta = 0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta = 0). Plot the loss over iterations for each method.\nWe’ll use the same data (x_exp, y_exp) to show what simply adding momentum can do. By changing our beta term, it adds a whole new dimension for equation (2) in logistic.py. The equation\n\\[\\begin{aligned}\n    \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\\]\nwill now have a non-zero last term with beta, which incorporates past weights \\({w}_{k-1}\\) to add momentum.\nBy including past weights and present weights to calculate future weights, the momentum term helps converge faster, and we can see this by using the train_and_plot function. Again, the only thing we’ve changed is the beta term from 0 to 0.9\n\ntrain_and_plot(X_exp, y_exp, 0.1, 0.9, 400)\n\n\n\n\n\n\n\n\nWe can see that the Gradient Descent with Momentum converges 25 times faster, taking only 400 iterations while the version above with no momentum took 10,000.\n\n\nOverfitting:\nLet’s generate some data where p_dim &gt; n_points. We’ll do this twice with the exact same parameters, the first for a train set and the second for a test set.\n\npoints = 200\ndims = 1000\n\nX_train, y_train = classification_data(n_points=points, noise=0.5, p_dims=dims)\nX_test, y_test = classification_data(n_points=points, noise=0.5, p_dims=dims)\n\nWe’ll then do an experiment in which we fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data.\n\n# create our logistic regression and optimizer\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\ntrain_acc = 0\n\n# train until we achieve 100% accuracy :0\nwhile (train_acc != 1):\n    \n    opt.step(X_train, y_train, 0.1, 0.9)\n\n    # use LogisticRegression.predict method to calculate accuracy\n    train_acc = (1.0*(LR.predict(X_train)==y_train)).mean()\n\n\nprint(\"training accuracy: \", float(train_acc))\nprint(\"testing accuracy: \", float((1.0*(LR.predict(X_test) == y_test)).mean()))\n\ntraining accuracy:  1.0\ntesting accuracy:  0.7799999713897705\n\n\nWhile it depends to a certain degree on the level of noise in the data, we can see that our testing accuracy is considerable lower than training accuracy. This is because using data where the number of features outweighs the number of data points results in overfitting.\nBecause there are so many features, the logistic regression model will essential memorize the data, rather than generalize from it - this is the way overfitting works. On new test data the model hasn’t seen, it isn’t as equipped to generalize to these new data points.\nIf we use the exact same pipeline, but switch the number of dimensions with the number of data points, we can see our test accuracy skyrocket.\n\npoints = 1000\ndims = 200\n\nX_train, y_train = classification_data(n_points=points, noise=0.5, p_dims=dims)\nX_test, y_test = classification_data(n_points=points, noise=0.5, p_dims=dims)\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\ntrain_acc = 0\n\nwhile (train_acc != 1):\n    \n    opt.step(X_train, y_train, 0.1, 0.9)\n    train_acc = (1.0*(LR.predict(X_train)==y_train)).mean()\n\nprint(\"training accuracy: \", float(train_acc))\nprint(\"testing accuracy: \", float((1.0*(LR.predict(X_test) == y_test)).mean()))\n\ntraining accuracy:  1.0\ntesting accuracy:  0.9819999933242798"
  },
  {
    "objectID": "posts/implementing-logistic-regression/index.html#discussion",
    "href": "posts/implementing-logistic-regression/index.html#discussion",
    "title": "Implementing Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we implemented Logistic Regression and a Gradient Descent with Momentum optimizer. This was a great way to cement my knowledge about logistic regression and learn the intricacies of the loss, grad, and step functions necessary to make these models and training loops work. The experiments showed the effects of momentum (the beta value), the relationship between number of points and number of dimensions, and visualized how Gradient Descent minimizes loss over the iteration cycle. I can now confidently describe the parameters, inner workings, and outputs of Gradient Descent."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Deep Music Genre Classification\n\n\n\n\n\nUsing Neural Networks to Classify Music Genres\n\n\n\n\n\nMay 16, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\nImplementing Newton’s Method for Logistic Regression\n\n\n\n\n\nMay 15, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nMay 12, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\nMar 7, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Blog\n\n\n\n\n\nPalmer Penguins!\n\n\n\n\n\nFeb 19, 2024\n\n\nCharlie Moore\n\n\n\n\n\n\nNo matching items"
  }
]